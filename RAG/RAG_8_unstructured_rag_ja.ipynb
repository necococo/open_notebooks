{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **非構造化RAG**\n",
    "非構造化または（準構造化）RAGは、テキスト、表、画像を組み合わせた文書を処理するために設計された手法です。テキストの分割による表の分断や、意味検索のための表の埋め込みの難しさといった課題に対応します。\n",
    "\n",
    "ここでは、テキスト、表、画像を解析して分離するために、`unstructured.io`を使用しています。\n",
    "\n",
    "**ツール参照:** [Unstructured](https://unstructured.io/)\n",
    "\n",
    "---\n",
    "## 環境構築 (macOS)\n",
    "\n",
    "以下のツールを環境にインストールします。\n",
    "\n",
    "```\n",
    "brew install libmagic poppler libreoffice pandoc tesseract\n",
    "```\n",
    "\n",
    "`unstructured` のインストール:\n",
    "*   [unstructured.io インストールガイド](https://docs.unstructured.io/open-source/installation/full-installation)\n",
    "\n",
    "```\n",
    "pip install \"unstructured[all-docs]\"\n",
    "```\n",
    "\n",
    "**対応ドキュメント形式:**\n",
    "\n",
    "\"csv\", \"doc\", \"docx\", \"epub\", \"image\", \"md\", \"msg\", \"odt\", \"org\", \"pdf\", \"ppt\", \"pptx\", \"rtf\", \"rst\", \"tsv\", \"xlsx\"\n",
    "\n",
    "---\n",
    "\n",
    "## 特定のデータコネクタのインストール (例: S3):\n",
    "\n",
    "```\n",
    "pip install \"unstructured-ingest[s3]\" # 今回は不使用\n",
    "```\n",
    "\n",
    "**データコネクタとは:**\n",
    "\n",
    "様々なデータソースからデータを取得し、アプリケーションやシステムで利用できるようにするためのツールまたはコンポーネントです。データの形式やアクセス方法の違いを吸収します。\n",
    "\n",
    "**対応データコネクタ:**\n",
    "\"airtable\", \"azure\", \"azure-ai-search\", \"biomed\", \"box\", \"confluence\", \"couchbase\", \"delta-table\", \"discord\", \"dropbox\", \"elasticsearch\", \"gcs\", \"github\", \"gitlab\", \"google-drive\", \"jira\", \"mongodb\", \"notion\", \"opensearch\", \"onedrive\", \"outlook\", \"reddit\", \"s3\", \"sharepoint\", \"salesforce\", \"slack\", \"wikipedia\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \"unstructured[all-docs]\" pdfplumber ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFから画像、テーブル、チャンク化テキストを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and extract images, tables, and chunk text\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "dataset_dir = \"../data/pdf/\"\n",
    "# pdf_file_name = \"57_public_スタートアップ育成に向けた政府の取組_file_name=kaisetsushiryou_2024.pdf\" # ページと図が多くテストに向いていないので変更\n",
    "pdf_file_name = \"Attention Is All You Need 1706.03762v7.pdf\"\n",
    "extract_image_block_output_dir = os.path.join(\"../data/\", \"test_images\")\n",
    "os.makedirs(extract_image_block_output_dir, exist_ok=True)\n",
    "\n",
    "pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(dataset_dir, pdf_file_name),\n",
    "#   chunking_strategy=\"by_title\",\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    # max_characters=4000,\n",
    "    # combine_text_under_n_chars=200,\n",
    "    extract_image_block_output_dir=extract_image_block_output_dir, # defaultでは実行ファイルがあるフォルダ下にfiguresが作成される\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [unstructured.partition.pdf のパラメータ](https://docs.unstructured.io/open-source/core-functionality/partitioning)\n",
    "\n",
    "### 主なパラメータ\n",
    "\n",
    "| パラメータ | 説明 |\n",
    "|---|---|\n",
    "| `filename` | PDF ファイルのパスを指定します。 |\n",
    "| `file` | PDF ファイルオブジェクトを指定します。`filename` または `file` のいずれかを指定する必要があります。 |\n",
    "| `strategy` | PDF ファイルの分割戦略を指定します。 |\n",
    "|  | `\"auto\"`: 自動的に最適な戦略を選択します (デフォルト)。 |\n",
    "|  | `\"fast\"`: 高速な分割を行います。 |\n",
    "|  | `\"hi_res\"`: 高解像度な分割を行います。 |\n",
    "|  | `\"ocr_only\"`: OCR (光学文字認識) のみを行います。 |\n",
    "| `infer_table_structure` | テーブル構造を推論するかどうかを指定します (`True`/`False`)。 |\n",
    "| `ocr_languages` | OCR で使用する言語を指定します。 |\n",
    "| `encoding` | ファイルのエンコーディングを指定します。 |\n",
    "| `include_page_breaks` | ページ区切りを含めるかどうかを指定します (`True`/`False`)。 |\n",
    "| `max_partition` | 最大分割数を指定します。 |\n",
    "| `extract_image_block_output_dir` | 画像の出力フォルダを指定します。 |\n",
    "\n",
    "### 詳細なパラメータ\n",
    "\n",
    "| パラメータ | 説明 |\n",
    "|---|---|\n",
    "| `chunking_strategy` | 分割された要素をどのようにチャンクにまとめるかを指定します。 |\n",
    "|  | `\"by_title\"`: タイトルごとにチャンクをまとめます。 |\n",
    "|  | `\"by_paragraph\"`: 段落ごとにチャンクをまとめます。 |\n",
    "|  | `\"contiguous\"`: 連続する要素をまとめてチャンクにします。 |\n",
    "| `new_after_n_words` | チャンクの最大単語数を指定します。 |\n",
    "| `new_after_n_pages` | チャンクの最大ページ数を指定します。 |\n",
    "| `metadata_filename` | メタデータファイルを指定します。 |\n",
    "| `pdf_text_extraction` | PDF テキスト抽出方法を指定します。 |\n",
    "|  | `\"textract\"`: textract ライブラリを使用します。 |\n",
    "|  | `\"tika\"`: Tika サーバーを使用します。 |\n",
    "|  | `\"paddlepaddle\"`: PaddlePaddle OCR を使用します。 |\n",
    "\n",
    "### パラメータの選択例\n",
    "\n",
    "| 例 | 設定 |\n",
    "|---|---|\n",
    "| 高速な分割 | `strategy=\"fast\"` |\n",
    "| 高解像度な分割 | `strategy=\"hi_res\"` |\n",
    "| テーブル構造の推論 | `infer_table_structure=True` |\n",
    "| 英語と日本語の OCR | `ocr_languages=[\"en\", \"ja\"]` |\n",
    "| ページ区切りを含める | `include_page_breaks=True` |\n",
    "\n",
    "### 注意点\n",
    "\n",
    "* パラメータの組み合わせによっては、期待通りの結果が得られない場合があります。\n",
    "* PDF ファイルの内容や構造によっては、分割がうまくいかない場合があります。\n",
    "* OCR を使用する場合、言語によっては精度が低い場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check unique categories\n",
    "\n",
    "from collections import Counter\n",
    "category_counts = Counter(str(type(element)) for element in pdf_elements)\n",
    "unique_categories = set(category_counts)\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique types\n",
    "unique_types = {el.to_dict()['type'] for el in pdf_elements}\n",
    "unique_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatGPTのAPI料金が高いので安いGoogle系に変更　（設定は面倒）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GCPの任意のプロジェクトでvertexAI APIを有効化[こちら](https://github.com/shohei0990/Multimodal_RAG_00/tree/main?tab=readme-ov-file)を参考にした。(私の場合は.envにAPIキーを記載)\n",
    "2. ローカルマシンにgoogle-cloud-sdkインストール\n",
    "- macOSの場合、Homebrewを使ってインストール\n",
    "brew install google-cloud-sdk\n",
    "1. 初期設定と認証\n",
    "- 初期化コマンドを実行\n",
    "gcloud init\n",
    "- Googleアカウントでログイン\n",
    "- プロジェクトを選択または新規作成\n",
    "1. インストール確認\n",
    "gcloud version  # バージョン確認\n",
    "cloud auth list  # 認証状態確認\n",
    "1. プロジェクト確認\n",
    "gcloud projects list  # プロジェクト一覧表示\n",
    "- 実行結果：\n",
    "PROJECT_ID: my-project-xxxx-xxxx PROJECT_NUMBER: xxxxxxxx\n",
    "1. リージョンとゾーンの設定\n",
    "- 東京リージョンを設定\n",
    "gcloud config set compute/region asia-northeast1\n",
    "- 具体的なゾーンを設定\n",
    "gcloud config set compute/zone asia-northeast1-a\n",
    "\n",
    "これで、Google Cloud SDKのインストールから設定まで完了し、Notebookから利用できる状態になった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install -qU google-cloud-aiplatform vertexai langchain_google_vertexai chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🔸 Multimodal Embedding を使いたい場合（画像+テキスト）**\n",
    "🔹 **現時点で Gemini AI Studio API ではマルチモーダルの埋め込みは未対応！**  \n",
    "🔹 **GCP の Vertex AI API を使う必要があるのだ！**  \n",
    "\n",
    "| **方法** | **Gemini AI Studio API** | **Vertex AI API (GCP)** |\n",
    "|------------|-------------------|----------------|\n",
    "| **テキスト埋め込み** | ✅ 可能（`models/embedding-001`） | ✅ 可能（`textembedding-gecko`） |\n",
    "| **画像の埋め込み** | ❌ 未対応 | ❌ 未対応 |\n",
    "| **画像 + テキスト埋め込み** | ❌ 直接は不可（画像→テキスト→埋め込み） | ❌ 直接は不可 |\n",
    "| **商用利用** | ❌ 制限あり | ✅ 可能 |\n",
    "| **おすすめ用途** | ✅ 個人開発 & プロトタイプ | ✅ 企業向け & 本番環境 |\n",
    "\n",
    "🚀 **結論**  \n",
    "🔹 **「Gemini AI Studio API」では、画像の埋め込みは直接できないが、画像 → テキスト → 埋め込み の方法なら可能！**  \n",
    "🔹 **マルチモーダル埋め込みが必要なら、OpenAI CLIP など他の選択肢も検討するのだ！**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-modal-RAGにおいてgeminiAI/VertexAIのapiを使うにはGCPで(面倒な)設定をする必要がある様子。\n",
    "# こちらを参考にした。https://github.com/shohei0990/Multimodal_RAG_00/tree/main?tab=readme-ov-file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import vertexai\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 環境変数 GOOGLE_APPLICATION_CREDENTIALS を設定\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.expanduser(\"~/.config/gcloud/application_default_credentials.json\") # たぶんgoogle-cloud-SDKを\"~\"にインストールしたのでこのファイルがここにできたのだと思う。\n",
    "vertexai.init(project=os.getenv(\"gcp_project_id\"), location=\"us-central1\")\n",
    "genai.configure(api_key=os.getenv(\"VERTEXAI_API_KEY\"))\n",
    "\n",
    "# for m in genai.list_models():\n",
    "#   if 'generateContent' in m.supported_generation_methods:\n",
    "#     print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "図は、スケール付きドット積注意機構（Scaled Dot-Product Attention）とマルチヘッド注意機構（Multi-Head Attention）のアーキテクチャを示しています。\n",
      "\n",
      "**左側の図：スケール付きドット積注意機構**\n",
      "\n",
      "この図は、スケール付きドット積注意機構の処理手順を段階的に示しています。\n",
      "\n",
      "1. **Q, K, V:** クエリ（Q）、キー（K）、バリュー（V）の3つの行列が入力されます。これらは、通常、入力シーケンスのエンコーディング表現です。\n",
      "2. **MatMul:** Q と K の行列積を計算します。これは、クエリとキー間の類似度を計算するステップです。\n",
      "3. **Scale:** 行列積をスケール係数で除算します。これは、大きな値によるSoftmax関数の勾配消失問題を防ぐために行われます。\n",
      "4. **Mask (opt.):** 必要に応じて、マスクを適用します。これは、シーケンスの将来の情報を使用しないようにする際に用いられます（例：自己回帰モデル）。\n",
      "5. **SoftMax:** 行列の各要素を確率に変換します。これは、各クエリに対するキーの重み付けを行います。\n",
      "6. **MatMul:** 重み付けされたキー（SoftMaxの結果）とVの行列積を計算します。これにより、クエリに対するコンテキストベクトルが生成されます。\n",
      "\n",
      "\n",
      "**右側の図：マルチヘッド注意機構**\n",
      "\n",
      "この図は、複数のスケール付きドット積注意機構を並列に実行するマルチヘッド注意機構を示しています。\n",
      "\n",
      "1. **Linear:** 入力（V, K, Q）を線形変換します。これは、異なる特徴空間で注意機構を実行するために行われます。\n",
      "2. **Scaled Dot-Product Attention:** 複数の（図では3つ）スケール付きドット積注意機構を並列に実行します。各機構は、異なる線形変換された入力を使用します。\n",
      "3. **Concat:** 各ヘッドからの出力を連結します。\n",
      "4. **Linear:** 連結された出力を線形変換して、最終的な出力ベクトルを生成します。\n",
      "\n",
      "\n",
      "要約すると、マルチヘッド注意機構は、複数のスケール付きドット積注意機構を組み合わせることで、入力シーケンスの様々な側面を捉え、より表現力豊かな注意機構を実現しています。  `h` はヘッドの数です。\n"
     ]
    }
   ],
   "source": [
    "# checking api, とりあえずレスポンスが文字で出力されるモデルでテスト、画像の埋め込みはあとで\"textembedding-gecko-multimodal\"に変える\n",
    "\n",
    "llm = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 画像の処理例\n",
    "from PIL import Image\n",
    "image_path = \"../data/test_images/figure-4-2.jpg\"\n",
    "image = Image.open(image_path)\n",
    "response = llm.generate_content(\n",
    "    [\"画像の説明を日本語でしてください\", image]  # image_dataはPILImage, バイト列, または画像のパスです\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud auth application-default login # 最初の一回だけ必要っぽい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU langchain_chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.PDFドキュメントのロード: UnstructuredPDFLoaderを使用して、PDFドキュメントをロードし、テキスト、画像、テーブルなどの要素を抽出します。\n",
    "2.画像の説明文の生成: Gemini 1.5 Flashを使用して、画像の説明文を生成します。\n",
    "3.ChromaDBへの登録: テキスト、画像の説明文、テーブルの内容をChromaDBに登録します。テキストの埋め込みには、textembedding-gecko-multimodalを使用します。私の環境では1.5h以上がかかりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "\n",
    "llm = VertexAI(model_name=\"gemini-1.5-flash\", temperature=0.2, max_output_tokens=2048)\n",
    "# text_embeddings = VertexAIEmbeddings(model_name=\"multimodalembedding\")\n",
    "text_embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko\")\n",
    "\n",
    "# ドキュメントのロード\n",
    "pdf_path = \"../data/pdf/Attention Is All You Need 1706.03762v7.pdf\"\n",
    "loader = UnstructuredPDFLoader(pdf_path, mode=\"elements\", strategy=\"fast\") # 前のセルでテストしたpartition_pdf関数がloader.load()の中で間接的に使用されています。\n",
    "documents = loader.load()\n",
    "\n",
    "# テキストの分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 画像の説明文を生成\n",
    "def generate_image_descriptions(documents: List[Document]) -> List[str]:\n",
    "    image_descriptions = []\n",
    "    for doc in documents:\n",
    "        if \"image_path\" in doc.metadata:\n",
    "            image = Image.open(doc.metadata[\"image_path\"])\n",
    "            response = llm.generate_content(\n",
    "                [f\"この画像を詳しく説明してください。重要な特徴や要素を含めて説明してください。\", image]\n",
    "            )\n",
    "            image_descriptions.append(response.text)\n",
    "        else:\n",
    "            image_descriptions.append(\"\")\n",
    "    return image_descriptions\n",
    "\n",
    "image_descriptions = generate_image_descriptions(documents)\n",
    "\n",
    "# ChromaDBへの登録 (テキストと画像の説明文を組み合わせる)\n",
    "metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "# メタデータから複雑な型を削除\n",
    "# ドキュメントリストに対してfilter_complex_metadataを適用\n",
    "filtered_metadatas = []\n",
    "for doc in documents:\n",
    "    filtered_metadata = filter_complex_metadata([doc])  # ドキュメントをリストで渡す\n",
    "    filtered_metadatas.append(filtered_metadata[0].metadata if filtered_metadata else {})  # 結果からメタデータを取り出す\n",
    "\n",
    "metadatas = filtered_metadatas\n",
    "\n",
    "combined_texts = []\n",
    "for i, doc in enumerate(documents):\n",
    "    if doc.metadata.get(\"type\") == \"image\":\n",
    "        combined_texts.append(f\"{doc.page_content}\\n画像の説明: {image_descriptions[i]}\")\n",
    "    elif doc.metadata.get(\"type\") == \"table\":\n",
    "        # テーブルの要約文を生成するコードを追加 (例: LLMを使用)\n",
    "        table_summary = f\"テーブルの内容: {doc.page_content}\"  # プレースホルダー\n",
    "        combined_texts.append(f\"{doc.page_content}\\n{table_summary}\")\n",
    "    else:\n",
    "        combined_texts.append(doc.page_content)\n",
    "\n",
    "db = Chroma.from_texts(combined_texts, text_embeddings, metadatas=metadatas, persist_directory=\"../data/chroma_db\")\n",
    "# db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クォータとは、クラウドサービスなどのリソースの使用量に対する制限のことです。\n",
    "\n",
    "クォータの目的:\n",
    "\n",
    "リソースの公平な分配: 全てのユーザーが公平にリソースを利用できるようにするため。\n",
    "システムの安定性維持: 特定のユーザーによる過剰なリソース消費を防ぎ、システム全体の安定性を維持するため。\n",
    "コスト管理: 意図しない過剰なリソース消費による高額な請求を防ぐため。\n",
    "セキュリティ: 悪意のあるユーザーによるリソースの不正利用を防ぐため。\n",
    "クォータの種類:\n",
    "\n",
    "クォータには、以下のような種類があります。\n",
    "\n",
    "時間あたりのリクエスト数: 1分あたり、1時間あたりなどのリクエスト数制限。\n",
    "ストレージ容量: データの保存容量制限。\n",
    "コンピューティングリソース: CPU、メモリなどの使用量制限。\n",
    "ネットワーク帯域幅: データ転送量制限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        question  \\\n",
      "0                     Transformerモデルの主要な特徴は何ですか？   \n",
      "1                   Attention Is All You Needの著者   \n",
      "2        TransformerモデルのAttention機構について説明してください。   \n",
      "3     Transformerモデルは、RNNやCNNと比較してどのような利点がありますか？   \n",
      "4                Transformerモデルの応用例をいくつか挙げてください。   \n",
      "5                        Transformerモデルの課題は何ですか？   \n",
      "6  この論文で提案されているTransformerモデルの計算コストについて説明してください。   \n",
      "\n",
      "                                            response  \\\n",
      "0  Transformerモデルの主要な特徴は、**自己注意メカニズム**です。これは、入力シー...   \n",
      "1  The authors of \"Attention Is All You Need\" are...   \n",
      "2  Transformerモデルは、マルチヘッドアテンションを3つの異なる方法で使用します。 \\...   \n",
      "3  Transformerモデルは、RNNやCNNと比較して、以下の利点があります。\\n\\n* ...   \n",
      "4                 Transformerモデルの応用例をいくつか挙げてください。 \\n   \n",
      "5  I don't know. I need more information about th...   \n",
      "6  I don't know. I need more context to answer th...   \n",
      "\n",
      "                                             context  \n",
      "0  [page_content='Figure 1: The Transformer - mod...  \n",
      "1  [page_content='Attention Is All You Need' meta...  \n",
      "2  [page_content='The Transformer uses multi-head...  \n",
      "3  [page_content='ByteNet [18] Deep-Att + PosUnk ...  \n",
      "4  [page_content='√' metadata={'category': 'Uncat...  \n",
      "5  [page_content='Figure 1: The Transformer - mod...  \n",
      "6  [page_content='√' metadata={'category': 'Uncat...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 質問応答: ユーザーからの質問を受け取り、ChromaDBから関連する情報を検索し、Gemini 1.5 Flashを使用して回答を生成します。\n",
    "queries = [\n",
    "    \"Transformerモデルの主要な特徴は何ですか？\",\n",
    "    \"Attention Is All You Needの著者\",\n",
    "    \"TransformerモデルのAttention機構について説明してください。\",\n",
    "    \"Transformerモデルは、RNNやCNNと比較してどのような利点がありますか？\",\n",
    "    \"Transformerモデルの応用例をいくつか挙げてください。\",\n",
    "    \"Transformerモデルの課題は何ですか？\",\n",
    "    \"この論文で提案されているTransformerモデルの計算コストについて説明してください。\"\n",
    "]\n",
    "questions = []\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "for query in queries:\n",
    "    result = qa({\"query\": query})\n",
    "    questions.append(query)\n",
    "    answers.append(result[\"result\"])\n",
    "    contexts.append(result[\"source_documents\"])\n",
    "    time.sleep(10)  # クォータ制限の上限に引っかかるので各クエリの後に待機 \n",
    "\n",
    "df = pd.DataFrame({\"question\": questions, \"response\": answers, \"context\": contexts})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "eval_llm = VertexAI(model_name=\"gemini-2.0-flash\", temperature=0.0, max_output_tokens=2048)\n",
    "\n",
    "relevance_evaluator = load_evaluator(\n",
    "    EvaluatorType.QA,\n",
    "    criteria=\"relevance\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    model_kwargs={\"temperature\": 0}\n",
    ")\n",
    "\n",
    "accuracy_evaluator = load_evaluator(\n",
    "    EvaluatorType.QA,\n",
    "    criteria=\"accuracy\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    model_kwargs={\"temperature\": 0}\n",
    ")\n",
    "\n",
    "def generate_synthetic_answer(question):\n",
    "    \"\"\"LLMを使用して質問に対する正解データを生成します.\"\"\"\n",
    "    prompt = f\"質問: {question}\\n 正解データ:\"\n",
    "    response = eval_llm.invoke(prompt).strip()\n",
    "    return response.strip()\n",
    "\n",
    "def evaluate_response(df):\n",
    "    \"\"\"\n",
    "    与えられたデータリストに対する Relevance と Accuracy の評価を行い、結果を DataFrame で返します。\n",
    "    \"\"\"\n",
    "    def _flatten_context(context_val):\n",
    "        if isinstance(context_val, list):\n",
    "            return \"\\n\".join([item.page_content if hasattr(item, \"page_content\") else str(item) for item in context_val])\n",
    "        return str(context_val)\n",
    "    \n",
    "    df[\"ContextText\"] = df[\"context\"].apply(lambda x: _flatten_context(x))\n",
    "    df[\"target_answer\"] = df[\"question\"].apply(generate_synthetic_answer) # 疑似的な正解データを生成\n",
    "\n",
    "    def _evaluate_row(row):\n",
    "        relevance_score = relevance_evaluator.evaluate_strings(\n",
    "            prediction=row[\"response\"],\n",
    "            input=row[\"question\"],\n",
    "            reference=row[\"ContextText\"]\n",
    "        )\n",
    "        accuracy_score = accuracy_evaluator.evaluate_strings(\n",
    "            prediction=row[\"response\"],\n",
    "            input=row[\"question\"],\n",
    "            reference=row[\"target_answer\"]\n",
    "        )\n",
    "        return relevance_score, accuracy_score\n",
    "    \n",
    "    df[[\"Relevance\", \"Accuracy\"]] = df.apply(lambda row: pd.Series(_evaluate_row(row)), axis=1)\n",
    "    \n",
    "    df[\"RelevanceScore\"] = df[\"Relevance\"].apply(lambda x: 1 if x[\"value\"].upper() == \"CORRECT\" else 0)\n",
    "    df[\"AccuracyScore\"] = df[\"Accuracy\"].apply(lambda x: 1 if x[\"value\"].upper() == \"CORRECT\" else 0)\n",
    "    \n",
    "    # 平均スコアを計算\n",
    "    avg_relevance = df[\"RelevanceScore\"].mean()\n",
    "    avg_accuracy = df[\"AccuracyScore\"].mean()\n",
    "    \n",
    "    print(f\"Average Relevance: {avg_relevance:.2f}\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.2f}\")\n",
    "\n",
    "    # きれいに処理したContextTextカラムがあるので、その前段階のcontextカラムを削除して返す\n",
    "    return df.drop(columns=['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/5c7mgh4j7j9clvvpsvwjbvhr0000gn/T/ipykernel_3764/3677457431.py:24: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = eval_llm(prompt)  # eval_llmを使用\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relevance: 0.29\n",
      "Average Accuracy: 0.43\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>ContextText</th>\n",
       "      <th>target_answer</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>RelevanceScore</th>\n",
       "      <th>AccuracyScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformerモデルの主要な特徴は何ですか？</td>\n",
       "      <td>Transformerモデルの主要な特徴は、**自己注意メカニズム**です。これは、入力シー...</td>\n",
       "      <td>Figure 1: The Transformer - model architecture...</td>\n",
       "      <td>Transformerモデルの主要な特徴は以下の通りです。\\n\\n*   **Attenti...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Attention Is All You Needの著者</td>\n",
       "      <td>The authors of \"Attention Is All You Need\" are...</td>\n",
       "      <td>Attention Is All You Need\\nAttention Is All Yo...</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
       "      <td>{'reasoning': 'CORRECT', 'value': 'CORRECT', '...</td>\n",
       "      <td>{'reasoning': 'CORRECT', 'value': 'CORRECT', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransformerモデルのAttention機構について説明してください。</td>\n",
       "      <td>Transformerモデルは、マルチヘッドアテンションを3つの異なる方法で使用します。 \\...</td>\n",
       "      <td>The Transformer uses multi-head attention in t...</td>\n",
       "      <td>TransformerモデルのAttention機構は、入力シーケンス内の各単語（またはトー...</td>\n",
       "      <td>{'reasoning': 'CORRECT', 'value': 'CORRECT', '...</td>\n",
       "      <td>{'reasoning': 'CORRECT', 'value': 'CORRECT', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformerモデルは、RNNやCNNと比較してどのような利点がありますか？</td>\n",
       "      <td>Transformerモデルは、RNNやCNNと比較して、以下の利点があります。\\n\\n* ...</td>\n",
       "      <td>ByteNet [18] Deep-Att + PosUnk [39] GNMT + RL ...</td>\n",
       "      <td>Transformerモデルは、RNNやCNNと比較して、以下の点で大きな利点があります。\\...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>{'reasoning': 'CORRECT', 'value': 'CORRECT', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformerモデルの応用例をいくつか挙げてください。</td>\n",
       "      <td>Transformerモデルの応用例をいくつか挙げてください。 \\n</td>\n",
       "      <td>√\\n√\\n√</td>\n",
       "      <td>Transformerモデルは、その並列処理能力と注意機構により、様々な分野で優れた性能を発...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformerモデルの課題は何ですか？</td>\n",
       "      <td>I don't know. I need more information about th...</td>\n",
       "      <td>Figure 1: The Transformer - model architecture...</td>\n",
       "      <td>Transformerモデルは非常に強力ですが、いくつかの課題も抱えています。主な課題として...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>この論文で提案されているTransformerモデルの計算コストについて説明してください。</td>\n",
       "      <td>I don't know. I need more context to answer th...</td>\n",
       "      <td>√\\n√\\n√</td>\n",
       "      <td>この論文で提案されているTransformerモデルの計算コストは、主に以下の2つの要因によ...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>{'reasoning': 'INCORRECT', 'value': 'INCORRECT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question  \\\n",
       "0                     Transformerモデルの主要な特徴は何ですか？   \n",
       "1                   Attention Is All You Needの著者   \n",
       "2        TransformerモデルのAttention機構について説明してください。   \n",
       "3     Transformerモデルは、RNNやCNNと比較してどのような利点がありますか？   \n",
       "4                Transformerモデルの応用例をいくつか挙げてください。   \n",
       "5                        Transformerモデルの課題は何ですか？   \n",
       "6  この論文で提案されているTransformerモデルの計算コストについて説明してください。   \n",
       "\n",
       "                                            response  \\\n",
       "0  Transformerモデルの主要な特徴は、**自己注意メカニズム**です。これは、入力シー...   \n",
       "1  The authors of \"Attention Is All You Need\" are...   \n",
       "2  Transformerモデルは、マルチヘッドアテンションを3つの異なる方法で使用します。 \\...   \n",
       "3  Transformerモデルは、RNNやCNNと比較して、以下の利点があります。\\n\\n* ...   \n",
       "4                 Transformerモデルの応用例をいくつか挙げてください。 \\n   \n",
       "5  I don't know. I need more information about th...   \n",
       "6  I don't know. I need more context to answer th...   \n",
       "\n",
       "                                         ContextText  \\\n",
       "0  Figure 1: The Transformer - model architecture...   \n",
       "1  Attention Is All You Need\\nAttention Is All Yo...   \n",
       "2  The Transformer uses multi-head attention in t...   \n",
       "3  ByteNet [18] Deep-Att + PosUnk [39] GNMT + RL ...   \n",
       "4                                            √\\n√\\n√   \n",
       "5  Figure 1: The Transformer - model architecture...   \n",
       "6                                            √\\n√\\n√   \n",
       "\n",
       "                                       target_answer  \\\n",
       "0  Transformerモデルの主要な特徴は以下の通りです。\\n\\n*   **Attenti...   \n",
       "1  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...   \n",
       "2  TransformerモデルのAttention機構は、入力シーケンス内の各単語（またはトー...   \n",
       "3  Transformerモデルは、RNNやCNNと比較して、以下の点で大きな利点があります。\\...   \n",
       "4  Transformerモデルは、その並列処理能力と注意機構により、様々な分野で優れた性能を発...   \n",
       "5  Transformerモデルは非常に強力ですが、いくつかの課題も抱えています。主な課題として...   \n",
       "6  この論文で提案されているTransformerモデルの計算コストは、主に以下の2つの要因によ...   \n",
       "\n",
       "                                           Relevance  \\\n",
       "0  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...   \n",
       "1  {'reasoning': 'CORRECT', 'value': 'CORRECT', '...   \n",
       "2  {'reasoning': 'CORRECT', 'value': 'CORRECT', '...   \n",
       "3  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...   \n",
       "4  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...   \n",
       "5  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...   \n",
       "6  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...   \n",
       "\n",
       "                                            Accuracy  RelevanceScore  \\\n",
       "0  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...               0   \n",
       "1  {'reasoning': 'CORRECT', 'value': 'CORRECT', '...               1   \n",
       "2  {'reasoning': 'CORRECT', 'value': 'CORRECT', '...               1   \n",
       "3  {'reasoning': 'CORRECT', 'value': 'CORRECT', '...               0   \n",
       "4  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...               0   \n",
       "5  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...               0   \n",
       "6  {'reasoning': 'INCORRECT', 'value': 'INCORRECT...               0   \n",
       "\n",
       "   AccuracyScore  \n",
       "0              0  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              0  \n",
       "5              0  \n",
       "6              0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_response(df)\n",
    "results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# memo: llm.generate_content と llm.invoke の違い\n",
    "\n",
    "llmの定義\n",
    "from langchain_google_vertexai import VertexAI\n",
    "llm = VertexAI(model_name=\"xxx\", temperature=yyy, max_output_tokens=zzz)\n",
    "\n",
    "\n",
    "| 特徴             | llm.generate_content                               | llm.invoke                                     |\n",
    "| ---------------- | ----------------------------------------------------- | ------------------------------------------------- |\n",
    "| 役割             | 汎用的なコンテンツ生成                                | シンプルなテキスト生成                               |\n",
    "| 入力             | テキスト、画像など                                    | テキスト                                           |\n",
    "| 主な用途         | マルチモーダルモデル (例: Gemini)                       | テキストベースの LLM (例: GPT-3)                      |\n",
    "| 出力             | GenerationResponse オブジェクト                       | 生成されたテキスト                                 |\n",
    "| 複雑さ           | 低レベル、より細かい制御が可能                           | 高レベル、使いやすい                               |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
