{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 第7章: 単語ベクトル\n",
        "単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．\n",
        "60. 単語ベクトルの読み込みと表示\n",
        "61. 単語の類似度\n",
        "62. 類似度の高い単語10件\n",
        "63. 加法構成性によるアナロジー\n",
        "64. アナロジーデータでの実験\n",
        "65. アナロジータスクでの正解率\n",
        "66. WordSimilarity-353での評価\n",
        "67. k-meansクラスタリング\n",
        "68. Ward法によるクラスタリング\n",
        "69. t-SNEによる可視化\n"
      ],
      "metadata": {
        "id": "Q_URLI9tvI5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "60. 単語ベクトルの読み込みと表示<br>\n",
        "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル（300万単語・フレーズ，300次元）](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
      ],
      "metadata": {
        "id": "fRXzsmdovXen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAyKwbQXuQVB",
        "outputId": "e0ccc4a0-8409-426d-8631-59e74c76cc48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# 1. Google Newsで学習済みWord2Vecモデルをダウンロードし保存\n",
        "!gdown --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM -O /content/drive/MyDrive/Colab Notebooks/data/GoogleNews-vectors-negative300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoZB4mDNvx_Y",
        "outputId": "2eff5633-e2af-4635-8d58-fa53a8c37c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "From (redirected): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&confirm=t&uuid=862249d8-872b-4f97-99db-4973ce129bfc\n",
            "To: /content/GoogleNews-vectors-negative300.bin.gz\n",
            "100% 1.65G/1.65G [00:19<00:00, 86.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. モデルのロード（バイナリ形式で読み込み）\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "# 3. \"United States\"の単語ベクトルを取得\n",
        "vector = model['United_States']\n",
        "\n",
        "# 4. 結果表示\n",
        "print(\"United States の単語ベクトル（最初の10次元）:\")\n",
        "print(vector[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "avoutpnqxXZ7",
        "outputId": "bc0d8a5d-5ecf-4ebb-9d08-1f797deac58a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'KeyedVectors' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1a3a76d8a094>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2. モデルのロード（バイナリ形式で読み込み）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/data/GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 3. \"United States\"の単語ベクトルを取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'United_States'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KeyedVectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "61. 単語の類似度<br>\n",
        "“United States”と”U.S.”のコサイン類似度を計算せよ．"
      ],
      "metadata": {
        "id": "CHYynXBZy0PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Newsで学習済みのWord2Vecモデルを使って類似度計算\n",
        "similarity = model.similarity('United_States', 'U.S.')\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bj58LcuyzvH",
        "outputId": "7fb94c94-c972-4150-f74d-a88332283489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.73107743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "62. 類似度の高い単語10件<br>\n",
        "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ"
      ],
      "metadata": {
        "id": "p_GsOtJzzV4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Newsで学習済みのWord2Vecモデルを使って類似度計算\n",
        "similar_words = model.most_similar('United_States', topn=10)\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCfiqiHazYeP",
        "outputId": "81131859-4c45-4c21-b939-e5ff0f9b9a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unites_States:  0.7877\n",
            "Untied_States:  0.7541\n",
            "United_Sates:  0.7401\n",
            "U.S.:  0.7311\n",
            "theUnited_States:  0.6404\n",
            "America:  0.6178\n",
            "UnitedStates:  0.6167\n",
            "Europe:  0.6133\n",
            "countries:  0.6045\n",
            "Canada:  0.6019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "63. 加法構成性によるアナロジー<br>\n",
        "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
      ],
      "metadata": {
        "id": "AWXljVS_vIa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Spain = model['Spain']\n",
        "Madrid = model['Madrid']\n",
        "Athens = model['Athens']\n",
        "\n",
        "# Madrid - Spain + Athens\n",
        "result = Madrid - Spain + Athens\n",
        "\n",
        "# Google Newsで学習済みのWord2Vecモデルを使ってresultベクトルから類似度計算\n",
        "similar_words = model.similar_by_vector(result, topn=10)\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkRae_z70h_z",
        "outputId": "51240d52-27da-4eed-a369-17671c38db46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Athens:  0.8219\n",
            "Madrid:  0.5877\n",
            "Rome:  0.5468\n",
            "Athens_Greece:  0.5300\n",
            "Peania:  0.4843\n",
            "Athen:  0.4821\n",
            "Mykonos_Island:  0.4807\n",
            "Cairo:  0.4799\n",
            "Organizing_Committee_ATHOC:  0.4790\n",
            "Thessaloniki:  0.4779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "64. アナロジーデータでの実験<br>\n",
        "単語アナロジーの評価データをダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
      ],
      "metadata": {
        "id": "ZdI6WFEA11Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /content/drive/MyDrive/Colab\\ Notebooks/data http://download.tensorflow.org/data/questions-words.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECiEKnKU2Nrw",
        "outputId": "8ac9b253-35b4-4111-a6f1-bfe8a623e39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-29 05:27:44--  http://download.tensorflow.org/data/questions-words.txt\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.11.207, 108.177.12.207, 74.125.26.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.11.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603955 (590K) [text/plain]\n",
            "Saving to: ‘/content/drive/MyDrive/Colab Notebooks/data/questions-words.txt’\n",
            "\n",
            "\rquestions-words.txt   0%[                    ]       0  --.-KB/s               \rquestions-words.txt 100%[===================>] 589.80K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-12-29 05:27:44 (41.6 MB/s) - ‘/content/drive/MyDrive/Colab Notebooks/data/questions-words.txt’ saved [603955/603955]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/data\")\n",
        "# データを格納するリスト\n",
        "results = []\n",
        "\n",
        "# ファイルを読み込んで処理\n",
        "with open(data_dir / 'questions-words.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        if line.startswith(':'):\n",
        "            current_category = line.strip().replace(':', '')\n",
        "        else:\n",
        "            word1, word2, word3, word4 = line.strip().split()\n",
        "            # Google Newsで学習済みのWord2Vecモデルを使って類似度計算\n",
        "            try:\n",
        "                result = model.most_similar(positive=[word2, word3], negative=[word1], topn=1)\n",
        "                predicted_word, similarity = result[0]\n",
        "            except KeyError:\n",
        "                predicted_word, similarity = 'NONE', 0.\n",
        "\n",
        "            # 結果をリストに追加\n",
        "            results.append({\n",
        "                'category': current_category,\n",
        "                'word1': word1,\n",
        "                'word2': word2,\n",
        "                'word3': word3,\n",
        "                'word4': word4,\n",
        "                'predicted': predicted_word,\n",
        "                'similarity': similarity\n",
        "            })\n",
        "\n",
        "# DataFrameに変換\n",
        "df = pd.DataFrame(results)\n",
        "# CSVに保存\n",
        "csv_path = data_dir / 'questions-words-with-results.csv'\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"CSVに保存しました: {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUSa0pG8_nBA",
        "outputId": "ca3e7523-e8ff-47c1-afef-fe7ea0e61d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSVに保存しました: /content/drive/MyDrive/Colab Notebooks/data/questions-words-with-results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "65. アナロジータスクでの正解率<br>\n",
        "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
      ],
      "metadata": {
        "id": "CE3QTnKl0gUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_dir / 'questions-words-with-results.csv')\n",
        "print(df.head())\n",
        "\n",
        "def categorize(row):\n",
        "    if 'gram' in row['category'].lower():\n",
        "        return 'syntactic'\n",
        "    else:\n",
        "        return 'semantic'\n",
        "\n",
        "df['type'] = df.apply(categorize, axis=1)\n",
        "\n",
        "type_accuracy = df.groupby('type').apply(lambda x: (x['word4'] == x['predicted']).sum() / len(x), include_groups=False)\n",
        "\n",
        "# 結果を表示\n",
        "print(\"\\n意味的アナロジーと文法的アナロジーの正解率:\")\n",
        "print(type_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPNDh2RE9-X_",
        "outputId": "9a3b6a39-c7e6-4edc-b2b8-81a617783ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    category   word1   word2    word3        word4  \\\n",
            "0   capital-common-countries  Athens  Greece  Baghdad         Iraq   \n",
            "1   capital-common-countries  Athens  Greece  Bangkok     Thailand   \n",
            "2   capital-common-countries  Athens  Greece  Beijing        China   \n",
            "3   capital-common-countries  Athens  Greece   Berlin      Germany   \n",
            "4   capital-common-countries  Athens  Greece     Bern  Switzerland   \n",
            "\n",
            "     predicted  similarity  \n",
            "0        Iraqi    0.635187  \n",
            "1     Thailand    0.713767  \n",
            "2        China    0.723578  \n",
            "3      Germany    0.673462  \n",
            "4  Switzerland    0.491975  \n",
            "\n",
            "意味的アナロジーと文法的アナロジーの正解率:\n",
            "type\n",
            "semantic     0.730860\n",
            "syntactic    0.740047\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "66. WordSimilarity-353での評価<br>\n",
        "The WordSimilarity-353 Test Collectionの評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ"
      ],
      "metadata": {
        "id": "DTQi_1zTD220"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P data_dir https://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6sbzqkjD_z1",
        "outputId": "de85718c-ccc2-4aa4-edd8-319a8dead051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-29 08:48:16--  https://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving www.gabrilovich.com (www.gabrilovich.com)... 173.236.137.139\n",
            "Connecting to www.gabrilovich.com (www.gabrilovich.com)|173.236.137.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23257 (23K) [application/zip]\n",
            "Saving to: ‘data_dir/wordsim353.zip’\n",
            "\n",
            "wordsim353.zip      100%[===================>]  22.71K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-12-29 08:48:16 (1.23 MB/s) - ‘data_dir/wordsim353.zip’ saved [23257/23257]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "data_path = data_dir / \"wordsim353.zip\"\n",
        "extract_path = data_dir / \"wordsim353\"\n",
        "\n",
        "# ZIPファイルの解凍\n",
        "with zipfile.ZipFile(data_path, 'r') as zip_red:\n",
        "    zip_red.extractall(extract_path)\n",
        "\n",
        "print(\"解凍完了\")\n",
        "!ls data_dir / \"wordsim353\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1MbdhKHEPvq",
        "outputId": "6d7d340e-2cba-46d7-e246-c1d908f31eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "解凍完了\n",
            "ls: cannot access 'wordsim353': No such file or directory\n",
            "/:\n",
            "bin\t\t\t    dev    lib64\t\t     opt\t\trun   tools\n",
            "boot\t\t\t    etc    libx32\t\t     proc\t\tsbin  usr\n",
            "content\t\t\t    home   media\t\t     python-apt\t\tsrv   var\n",
            "cuda-keyring_1.0-1_all.deb  lib    mnt\t\t\t     python-apt.tar.xz\tsys\n",
            "datalab\t\t\t    lib32  NGC-DL-CONTAINER-LICENSE  root\t\ttmp\n",
            "\n",
            "data_dir:\n",
            "wordsim353.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# CSVの読み込み\n",
        "df = pd.read_csv(data_dir / 'wordsim353/combined.csv')\n",
        "\n",
        "results = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    word1 = row['Word 1']\n",
        "    word2 = row['Word 2']\n",
        "    human_similarity = row['Human (mean)']\n",
        "\n",
        "    try:\n",
        "        # Google Newsで学習済みのWord2Vecモデルを使って類似度計算\n",
        "        model_similarity = model.similarity(word1, word2)\n",
        "    except KeyError:\n",
        "        model_similarity = 0.\n",
        "\n",
        "    results.append({\n",
        "        'word1': word1,\n",
        "        'word2': word2,\n",
        "        'human_similarity': human_similarity,\n",
        "        'model_similarity': model_similarity\n",
        "    })\n",
        "\n",
        "result_df = pd.DataFrame(results)\n",
        "\n",
        "print(result_df.head())\n",
        "\n",
        "spearman_corr, _ = spearmanr(result_df['human_similarity'], result_df['model_similarity'])\n",
        "\n",
        "print(f\"スピアマン相関係数: {spearman_corr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EchJ7Y5JGY7u",
        "outputId": "39c71fb9-96c5-4383-fd31-1eaca021e4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      word1     word2  human_similarity  model_similarity\n",
            "0      love       sex              6.77          0.263938\n",
            "1     tiger       cat              7.35          0.517296\n",
            "2     tiger     tiger             10.00          1.000000\n",
            "3      book     paper              7.46          0.363463\n",
            "4  computer  keyboard              7.62          0.396392\n",
            "スピアマン相関係数: 0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "67. k-meansクラスタリング<br>\n",
        "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
      ],
      "metadata": {
        "id": "iwrJVL8yI5Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# modelはロード済のGoogle Newsで学習済みのWord2Vecモデル\n",
        "\n",
        "# 1. 国名リストの準備\n",
        "countries = [\n",
        "    'Japan', 'Germany', 'France', 'Italy', 'Brazil', 'Canada', 'India', 'China',\n",
        "    'Russia', 'Spain', 'Mexico', 'Netherlands', 'Turkey', 'Australia', 'Sweden',\n",
        "    'Norway', 'Argentina', 'South_Korea', 'United_States', 'Egypt', 'Nigeria',\n",
        "    'South_Africa', 'Pakistan', 'Iran', 'Iraq', 'Israel', 'Vietnam', 'Thailand',\n",
        "    'Poland', 'Greece', 'Portugal', 'Malaysia', 'Singapore', 'Denmark', 'Finland'\n",
        "]\n",
        "\n",
        "# 2. 国名の単語ベクトルを取得\n",
        "vectors = []\n",
        "valid_countries = []\n",
        "\n",
        "for country in countries:\n",
        "    try:\n",
        "        vectors.append(model[country])\n",
        "        valid_countries.append(country)  # モデルに存在する国だけリストに追加\n",
        "    except KeyError:\n",
        "        pass  # モデルに存在しない国はスキップ\n",
        "\n",
        "print(f\"モデルに存在する国数: {len(valid_countries)}\")\n",
        "\n",
        "# 3. k-meansクラスタリング (k=5)\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(vectors)\n",
        "\n",
        "# 5. 結果をDataFrameに格納\n",
        "df = pd.DataFrame({'country': valid_countries, 'cluster': clusters})\n",
        "\n",
        "# 6. 各クラスタの国を表示\n",
        "for i in range(k):\n",
        "    print(f\"\\nクラスタ {i + 1}:\")\n",
        "    print(df[df['cluster'] == i]['country'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPtD-9lPJFDu",
        "outputId": "0051296a-734a-44b9-ab62-1bf228d04e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルに存在する国数: 35\n",
            "\n",
            "クラスタ 1:\n",
            "['Canada' 'India' 'Australia' 'Nigeria' 'South_Africa' 'Pakistan'\n",
            " 'Malaysia' 'Singapore']\n",
            "\n",
            "クラスタ 2:\n",
            "['Netherlands' 'Sweden' 'Norway' 'Poland' 'Denmark' 'Finland']\n",
            "\n",
            "クラスタ 3:\n",
            "['Germany' 'France' 'Italy' 'Brazil' 'Spain' 'Argentina' 'Greece'\n",
            " 'Portugal']\n",
            "\n",
            "クラスタ 4:\n",
            "['Turkey' 'Egypt' 'Iran' 'Iraq' 'Israel']\n",
            "\n",
            "クラスタ 5:\n",
            "['Japan' 'China' 'Russia' 'Mexico' 'South_Korea' 'United_States' 'Vietnam'\n",
            " 'Thailand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "68. Ward法によるクラスタリング<br>\n",
        "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．\n",
        "\n",
        "    memo:\n",
        "    - Ward法は、階層型クラスタリング（Hierarchical Clustering）の一種で、「クラスタ内の分散を最小化しながらクラスタを統合する方法」です。\n",
        "\n",
        "    - 階層型クラスタリングとは？\n",
        "        - データポイントを階層的にクラスタリングしていく手法です。\n",
        "        - 初めは各データポイントを個別のクラスタとして扱い、徐々にクラスタを統合していきます。\n",
        "        - 結果として、**デンドログラム（樹形図）**が得られます。\n",
        "\n",
        "    - Ward法の特徴\n",
        "        - 統合基準：クラスタリング時に「クラスタ内誤差（分散）の増加が最も少ないクラスタを統合」します。\n",
        "        - 分散最小化アプローチ：各クラスタの分散が小さいほど、クラスタの密度が高く、データのまとまりが良いと判断されます。\n",
        "        - 他の手法（最短距離法、最長距離法など）よりも、均一なサイズのクラスタが得られやすいのが特徴です。"
      ],
      "metadata": {
        "id": "-ynmOi83Md-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "id": "hrbi3SSznOGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca877c2-952f-4a85-b3f9-75e236e74cc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize-matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/4.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from japanize-matplotlib) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize-matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.17.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120257 sha256=0484888376e984c633c17a9f7ad8b19b45dd4c008bc1922b89e328a7a6a97c4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/7a/6b/df1f79be9c59862525070e157e62b08eab8ece27c1b68fbb94\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from matplotlib import pyplot as plt\n",
        "import japanize_matplotlib\n",
        "# Ward法による階層型クラスタリング\n",
        "linkage_matrix = linkage(vectors, method='ward')\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(linkage_matrix, labels=valid_countries, leaf_rotation=90)\n",
        "plt.title('国名の階層クラスタリング（Ward法）')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "Yy9jKiteMavR",
        "outputId": "8a98931f-fa9d-43a9-f32b-41cbe2f4ce3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vectors' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4ff54b4596d5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjapanize_matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Ward法による階層型クラスタリング\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlinkage_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "69. t-SNEによる可視化<br>\n",
        "ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ．"
      ],
      "metadata": {
        "id": "cOSbkt2YPgjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "# t-SNEで2次元に次元削減\n",
        "# tsne = TSNE(n_components=2, random_state=42, perplexity=int(len(valid_countries) * 0.1), n_iter=5000) # perplexity：データ数の5%〜10%が適切とされます\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=10, n_iter=5000) # perplexity：データ数の5%〜10%が適切とされます\n",
        "vectors = np.array(vectors)\n",
        "tsne_vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "# クラスタごとに色分けして可視化\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(tsne_vectors[:, 0], tsne_vectors[:, 1], c=clusters, cmap='viridis', s=250, alpha=0.5)\n",
        "\n",
        "# 凡例（クラスタ番号）\n",
        "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "\n",
        "# 国名をプロット\n",
        "for i, country in enumerate(valid_countries):\n",
        "    plt.annotate(country, (tsne_vectors[i, 0], tsne_vectors[i, 1]))\n",
        "\n",
        "plt.title(\"国名のt-SNE可視化（クラスタリング付き）\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "WW-7bXIaPk7l",
        "outputId": "55608402-1ca6-4dd1-f882-07ad2eff022d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vectors' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-269f1c9f519a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# perplexity：データ数の5%〜10%が適切とされます\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtsne_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
          ]
        }
      ]
    }
  ]
}