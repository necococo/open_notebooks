## そもそも論: Long Context vs. RAG

[Long Context vs. RAG for LLMs: An Evaluation and Revisits](https://arxiv.org/abs/2501.01880)

【LongContext】は特にWikipediaや書籍などの構造化されたテキストで優れた性能を示し、一方で【RAG技術】は対話ベースのシナリオや断片的な情報処理において優位性を持つことが判明しました。【文脈理解】の正解率では、LongContext(LC)が56.3%、RAGが49.0%という具体的な結果が得られています。

またLCとRAGの[Hybrid](https://qiita.com/DDDDai/items/8f188fde488e4e38241a)も考えられますし、[GraphRAG](https://arxiv.org/html/2404.16130v2)や[RetroLLM](https://arxiv.org/html/2412.11919v1)などとのHybridも考えることはできます。(精度はわからない)

## 日本語RAGリーダーボード
[日本語 RAG Leaderboard (2024年9月13日公開)](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA?ref=blog-ja.allganize.ai) 


# Fine-tuningや評価に使えそうなデータセット
| データセット名                               | 種類         | データの質（人手チェック割合）                                                                                                                                  | 出典（提供元）                                                                                       | ライセンス <br>（説明）                                                                                         | 主な用途                                                                                         | 対応タスク（分野）                                                                    |
|--------------------------------------------|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| **JSQuAD (JGLUE)**                         | テキストQA  | SQuAD1.1に準拠した日本語読解QA。質問と回答は人手で作成され高品質。                                                                                     | Yahoo!日本法人・早稲田大学の共同研究（JGLUEプロジェクト）                                             | CC BY 4.0 <br> (クリエイティブ・コモンズ 表示4.0。商用利用・改変可、著作権表示必須)                  | 機械読解モデルの訓練・評価。チャットボットや検索システムのQA機能強化に利用。                      | 一般知識全般（百科事典知識、歴史、科学、芸術など）                                      |
| **JCommonsenseQA (JGLUE)**                 | テキストQA  | 常識推論が必要な多肢選択QA。ConceptNetをもとにクラウドソーシングで収集。                                                                                      | Yahoo!日本法人・早稲田大学の共同研究（JGLUEプロジェクト）                                             | CC BY 4.0 <br> (クリエイティブ・コモンズ 表示4.0。商用利用・改変可、著作権表示必須)                  | 常識推論QAのモデル訓練・評価。チャットボットの常識的応答能力評価に利用。                           | 一般常識（日常的な知識、生活シチュエーションの推論）                                    |
| **JAQKET**                                 | テキストQA  | オープンドメインのクイズ形式QA。Wikipediaの記事タイトルを正解とする択一問題を研究チームが作成。                                                              | 学術研究（言語処理学会第26回年次大会 2020発表）                                                       | CC BY-SA 4.0 <br> (表示-継承4.0。改変・商用利用可、同一条件再配布必須)                     | オープンドメイン質問応答のモデル訓練・評価。クイズアプリや知識検索システムの開発に利用。              | 一般知識（地理、歴史、人物、科学など）                                                 |
| **JaQuAD**                                | テキストQA  | 日本版SQuAD形式の抽出型QA。全39,696問答ペアを人間のアノテータが作成し非常に高品質。                                                                 | Skelter Labs社による提供（2022年公開）。Wikipedia記事から収集。                                      | CC BY-SA 3.0 <br> (表示-継承3.0。改変・商用利用可、同一条件再配布必須)                     | 機械読解QAモデルの訓練・評価。検索エンジンやQAチャットボットへの組み込みに利用。                     | 一般知識（Wikipedia由来の幅広い分野）                                                   |
| **JDocQA**                                 | VQA（文書） | ドキュメント画像＋テキストに対するQA。各文書ごとに2～4問のQAを人手で作成。官公庁資料PDFなどの公的文書から収集。                                            | 学術研究（LREC-COLING 2024、Onamiら）<br>国会図書館デジタルコレクションや官公庁資料PDFをソースとして使用。  | CC BY-SA 4.0 <br> (表示-継承4.0。改変・商用利用可、同一条件再配布必須)                     | 文書内の情報に基づく質問応答システムの開発。官公庁のPDF資料から自動で回答を生成するシステムに利用。       | 行政・政府分野（経済政策、教育政策、医療・衛生、農林水産、文化・歴史など）                 |
| **IgakuQA (医学QAベンチマーク)**           | テキストQA  | 厚生労働省の医師国家試験過去5年分の問題と正答を収集。専門家作成で信頼性が非常に高い。                                                    | 東京大学・ワシントン大学らの研究。厚生労働省の公開試験問題データベースから取得。                | 政府公開情報 <br> (公的機関提供、原則自由利用。商用利用制約なし)                     | 専門領域QAのモデル評価・訓練。医療問診チャットボットや教育ツールへの活用に利用。                      | 医療分野（医学知識、臨床判断など国家試験レベルの専門QA）                             |
| **JaGovFaqs-22k** [学習済モデル](https://huggingface.co/answerdotai/JaColBERTv2.5) | テキストQA  | 地方自治体の公文書やFAQを元に高品質なQAペアを構築。専門家による検証が行われている可能性が高い。                                                          | matsuxr（Hugging Face）<br>公式サイト: [JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) | CC BY 4.0 <br> (クリエイティブ・コモンズ 表示4.0。商用利用・改変可、著作権表示必須)                  | 官公庁・地方自治体のFAQシステム、行政情報検索チャットボットなど                                  | 政府FAQ、行政手続き、政策説明など                                                       |
| **allganize/RAG-Evaluation-Dataset-JA**    | テキストQA  | RAGシステムの評価用として厳選された高品質なQAペア。手作業による検証や調整が行われている。                                                                       | allganize（Hugging Face）                                                                            | CC BY 4.0 <br> (クリエイティブ・コモンズ 表示4.0。商用利用・改変可、著作権表示必須)                  | RAGシステムの評価・ベンチマーク、QAチャットボットの性能検証                                           | 政府・行政関連FAQ、RAG、QA評価                                                         |