# Diff-Transformer（Diff-Attention）の具体的計算例

作成日時: 2024年10月22日 14:51
クラスID: 情報整理
URL: https://arxiv.org/pdf/2410.05258

論文の擬似コードをみても、なぜ注意のノイズが少なくなるのかよくわからなかったので、たぶんこうなっているはずという計算をしてみました。

![スクリーンショット 2024-10-22 14.52.31.png](https://github.com/necococo/images/blob/main/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202024-11-26%2018.15.50.png?raw=true)

# 文章要約を例にしたDiff-Attention計算例

### 要約対象の文章

「AIは自然言語を含めた今後の技術発展に重要な役割を果たす。猫かわいい。」

ここからノイズを取って良い要約を得たいとします。

## ステップ1: アテンションマップの計算

Diff-Attentionのプロセスでは、2つの異なるアテンションマップが計算されデータを2つの観点から見比べることができるように学習されます。

ここでは、**A1**は「技術的観点」、**A2**は「感情的観点」に基づくアテンションマップとしてうまく学習されたとします。(softmaxによる正規化などは簡単のため省略。 )　

### アテンションマップA1（技術的観点からの重要度）

A1 = [AI=0.7, 自然言語=0.5, 今後=0.3, 技術発展=0.8, 重要=0.6, 猫かわいい=0.1]

- **A1**は「技術発展」や「AI」に高いアテンションが割り当てられ、技術的なキーワードに強く注目しています。

### アテンションマップA2（感情的観点からの重要度）

A2 = [AI=0.2, 自然言語=0.3, 今後=0.5, 技術発展=0.2, 重要=0.4, 猫かわいい=0.9]

- **A2**では「猫かわいい」に高いスコアが割り当てられています。

## ステップ2: Diff-Attentionの計算

次に、A1とA2の差分を取ります。ここではλ=0.5と仮定して計算を行います。

(λは一つのDiffAttn層に対し一つだけ学習されるスカラー値のパラメータです)

**Diff-Attention = A1 - λ * A2**

= [AI=0.6, 自然言語=0.35, 今後=0.05, 技術発展=0.7, 重要=0.4, 猫かわいい=-0.35]

## ステップ3: 結果の解釈

Diff-Attentionの結果から、各単語の最終的なスコアを以下のように解釈できます(0.5以上を残すとすれば)：

- *「AI」**や**「技術発展」**は、差分を取った後も高いスコア（**0.6**、**0.7**）が残りました。これらは要約に残る重要な情報です。
- **「猫かわいい」は感情的なアテンションで高く評価されましたが、技術的な視点ではほとんど無視され、最終的なスコアは-0.35**となり、要約からは除外されます。

## ステップ4: 要約結果

「AIは技術発展に重要な役割を果たす。」

元の文章と比較してみましょう。

「AIは自然言語を含めた今後の技術発展に重要な役割を果たす。猫かわいい。」

----

以上、論文の擬似コードを見て、こういう事なのかなと思いましたという記事です。
現在は、実際に言語モデルにDiff-Attenionを入れてA1,A2 mapの違いを可視化しようと実験中ですが、今のところなんだか微妙です。

----

ちなみに、Hugging faceのこの論文のページ([https://huggingface.co/papers/2410.05258](https://huggingface.co/papers/2410.05258))には

**A1とA2はどちらも学習可能なパラメーターで計算され、トレーニングの過程でお互いを「認識」できる。 その結果、損失がより少なくなるように、互いを調整することができる。 その結果、モデルはシグナルを維持し、ノイズを打ち消すことを選択する。 そして単一のソフトマックスでは、その定式化と勾配の特性により、同じ解を学習することは難しい。とありました。**