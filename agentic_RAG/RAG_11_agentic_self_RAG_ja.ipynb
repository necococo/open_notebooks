{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Self RAG**\n",
    "\n",
    "Self-RAGは、言語モデル（LM）によって生成されたテキストの精度と品質を向上させる手法です。関連情報を検索によって見つけ、モデルがその出力について振り返ることを可能にすることで、これを実現します。\n",
    "\n",
    "モデルは検索された文章の助けを借りてテキストを生成し、その後、振り返りトークンを作成することで自身の回答をチェックします。これらのトークンは、モデルに対して、より多くの情報が必要かどうか、または回答が完全で検索されたデータによって裏付けられているかどうかを伝えます。\n",
    "\n",
    "Self-RAGは、３つの採点者(Grader)を使います。\n",
    "- document grader: 検索してきた文書が証拠として使えるか使えないか\n",
    "- halucination grader: 回答に証拠があるかないか\n",
    "- answer grader: 回答に妥当性があるかないか\n",
    "\n",
    "Research Paper: [Self RAG](https://arxiv.org/pdf/2310.11511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# # for web search　今回は使わない\n",
    "# os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "# VERTEXAI用の設定\n",
    "import vertexai\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.expanduser(\"~/.config/gcloud/application_default_credentials.json\")\n",
    "vertexai.init(project=os.getenv(\"gcp_project_id\"), location=\"us-central1\")\n",
    "\n",
    "# load llm\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-exp\",\n",
    "    project=os.getenv(\"gcp_project_id\"),\n",
    "    location=\"us-central1\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# # load data\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"../data/pdf/57_public_スタートアップ育成に向けた政府の取組_file_name=kaisetsushiryou_2024.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either: - Avoid using tokenizers before the fork if possible - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" # 警告対策　tokenizersライブラリの並列処理を明示的にON \n",
    "\n",
    "# load embedding model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "from langchain.vectorstores import Chroma ## 何故か今日は以下のエラーが出てつかえなかった、機能は使えたのに。謎。仕方がないのでFAISSに変更した。\n",
    "# RuntimeError: Chroma is running in http-only client mode, and can only be run with 'chromadb.api.fastapi.FastAPI' or 'chromadb.api.async_fastapi.AsyncFastAPI' as the chroma_api_impl.             see https://docs.trychroma.com/guides#using-the-python-http-only-client for more information.\n",
    "# Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "# vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=\"../data/chroma_db_57\")\n",
    "# vectorstore = Chroma(persist_directory=\"../data/chroma_db_57\", embedding_function=embeddings, client_settings=settings)\n",
    "# vectorstore = Chroma(persist_directory=\"../data/chroma_db_57\", embedding_function=embeddings)\n",
    "\n",
    "# create retirever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Document Grader**\n",
    "ドキュメント評価者は、ドキュメントが与えられたクエリに関連しているかどうかを評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grader for doc retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# define a data class\n",
    "class GradeDocuments(BaseModel): \n",
    "    \"\"\"Schema for grading retrieved documents for relevance.\n",
    "    The field 'binary_score' is expected to be either \"yes\" or \"no\" indicating\n",
    "    whether the document is relevant to the user's question.\n",
    "    \"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt for the grader\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# testing grader\n",
    "question = \"沖縄科学技術大学院大学(OIST)のスタートアップ支援に関する予算額は？\"\n",
    "docs = retriever.invoke(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": docs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document chain\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\"\n",
    "You are a helpful assistant that answers questions based on the following context\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'沖縄科学技術大学院大学(OIST)のスタートアップ支援に関する予算額は、R4補正で23億円の内数、R5補正で26億円の内数、R6当初で196億円の内数(内閣府)です。\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Grader\n",
    "その答えが与えられた事実に基づいていたり、裏付けられているかどうかを確認する。\n",
    "\n",
    "(先程のDocument Graderは retrieve したドキュメントが与えられたクエリに関連しているかどうかを評価していた。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='no')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create grader for hallucination\n",
    "# define a data class\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# prompt for the grader\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Answer Grader**\n",
    "回答が与えられた質問に効果的に対処しているかどうかを評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='no')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create grader for answer\n",
    "# define a data class\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# prompt for the grader\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data class for state\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodeは update された state を返し、エッジは文字列を返します。\n",
    "\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "MAX_RETRIES = 3  \n",
    "\n",
    "# ノード\n",
    "def retrieve(state):\n",
    "    print(\"---関連文書の取得---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 取得\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        documents = retriever.invoke(question)\n",
    "        state[\"documents\"] = documents  # stateオブジェクトを直接変更\n",
    "    except Exception as e:\n",
    "        print(f\"取得中のエラー: {e}\")\n",
    "        state[\"documents\"] = []  # stateオブジェクトを直接変更\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---回答生成---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG生成\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "        state[\"generation\"] = generation  # stateオブジェクトを直接変更\n",
    "    except Exception as e:\n",
    "        print(f\"回答生成中のエラー: {e}\")\n",
    "        state[\"generation\"] = \"\"  # stateオブジェクトを直接変更\n",
    "    return state\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"---質問に対する文書の関連性を確認---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 各文書をスコアリング\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            score = retrieval_grader.invoke(\n",
    "                {\"question\": question, \"document\": d.page_content}\n",
    "            )\n",
    "            grade = score.binary_score\n",
    "            if grade == \"yes\":\n",
    "                print(\"---評価: 文書は関連しています---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---評価: 文書は関連していません---\")\n",
    "        except Exception as e:\n",
    "            print(f\"文書評価中のエラー: {e}\")\n",
    "            continue\n",
    "    state[\"documents\"] = filtered_docs  # stateオブジェクトを直接変更\n",
    "    return state\n",
    "\n",
    "\n",
    "# エッジ\n",
    "def decide_to_generate(state):\n",
    "    print(\"---OKな文書があれば回答を生成---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # すべての文書がフィルタリングされました\n",
    "        print(\"---決定: すべての文書は質問に関連していません---\")\n",
    "        return \"no_relevant_documents\"\n",
    "    else:\n",
    "        # 関連する文書があるので、回答を生成\n",
    "        print(\"---決定: 生成---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def check_hallucination(state):  # 幻覚チェックの関数を分離\n",
    "    print(\"---幻覚を確認---\")\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        score = hallucination_grader.invoke(\n",
    "            {\"documents\": documents, \"generation\": generation}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        return grade == \"yes\"\n",
    "    except Exception as e:\n",
    "        print(f\"幻覚評価中のエラー: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def evaluate_answer(state):  # 回答評価の関数を分離\n",
    "    print(\"---生成と質問を評価---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        return grade == \"yes\"\n",
    "    except Exception as e:\n",
    "        print(f\"回答評価中のエラー: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    print(f\"{retry_count=}\")\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        print(f\"---最大再試行回数({MAX_RETRIES})に達しました---\")\n",
    "        return \"max_retries_reached\"\n",
    "\n",
    "    if not check_hallucination(state):\n",
    "        print(\"---決定: 生成は文書に基づいていません、再試行---\")\n",
    "        state[\"retry_count\"] = retry_count + 1\n",
    "        return \"not supported\"\n",
    "\n",
    "    if not evaluate_answer(state):\n",
    "        print(\"---決定: 生成は質問に答えていません---\")\n",
    "        return \"not useful\"\n",
    "\n",
    "    print(\"---決定: 生成は文書に基づいており、質問に答えています---\")\n",
    "    return \"useful\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、langgraph ライブラリを使ってワークフローグラフを構成するシンプルな例です。最初に、グラフの基本状態（GraphState）を元に StateGraph を作成し、開始（START）および終了（END）の定数とともにワークフローを定義しています。\n",
    "\n",
    "主要な処理は「retrieve」「grade_documents」「generate」という 3 つのノードに分かれており、それぞれ対応する関数（retrieve、grade_documents、generate）を実行します。最初は START から「retrieve」へ、次に「retrieve」から「grade_documents」へと順に進みます。\n",
    "\n",
    "そこから、grade_documents の結果に応じて条件付きで分岐し、decide_to_generate 関数が返す結果が「generate」の場合は「generate」ノードに、関連文書がない場合はワークフローを終了（END）します。さらに、generate ノードの出力を grade_generation_v_documents_and_question 関数で評価し、結果に応じて再試行（\"not supported\"）や終了（\"not useful\"、\"max_retries_reached\"、\"useful\"）の判断を下します。\n",
    "\n",
    "最後に workflow.compile() で定義したグラフを実行可能なアプリケーション app にコンパイルしています。これにより、各ノードが順番に実行され、条件に基づいた分岐処理が行われるワークフローが完成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"no_relevant_documents\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"not useful\": END,\n",
    "        \"max_retries_reached\": END,\n",
    "        \"useful\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---関連文書の取得---\n",
      "\"Node 'retrieve':\"\n",
      "---質問に対する文書の関連性を確認---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連しています---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連していません---\n",
      "---OKな文書があれば回答を生成---\n",
      "---決定: 生成---\n",
      "\"Node 'grade_documents':\"\n",
      "---回答生成---\n",
      "retry_count=0\n",
      "---幻覚を確認---\n",
      "---生成と質問を評価---\n",
      "---決定: 生成は質問に答えていません---\n",
      "\"Node 'generate':\"\n",
      "'ディープテック分野への横展開（NEDO）の予算として、15億円の内数（令和６年度当初予算）と21億円の内数（令和７年度概算要求額）が記載されています。'\n"
     ]
    }
   ],
   "source": [
    "# example 2 (no relevant documents)\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = [\n",
    "    {\"question\": \"ディープテック・スタートアップの起業・経営人材確保等支援事業の予算額は？\"},\n",
    "]\n",
    "\n",
    "for inp in inputs:\n",
    "    for i, output in enumerate(app.stream(inp)):\n",
    "        for key, value in output.items():  # i=0 key='retrieve' value.keys()=dict_keys(['documents', 'question'])\n",
    "            pprint(f\"Node '{key}':\")\n",
    "\n",
    "    if \"generation\" in value:\n",
    "        pprint(value['generation'])\n",
    "    else:\n",
    "        pprint(\"関連文書が見つからない、または生成されない。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparing Data for Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---関連文書の取得---\n",
      "---質問に対する文書の関連性を確認---\n",
      "---評価: 文書は関連しています---\n",
      "---評価: 文書は関連しています---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連していません---\n",
      "---OKな文書があれば回答を生成---\n",
      "---決定: 生成---\n",
      "---回答生成---\n",
      "retry_count=0\n",
      "---幻覚を確認---\n",
      "---生成と質問を評価---\n",
      "---決定: 生成は文書に基づいており、質問に答えています---\n",
      "---関連文書の取得---\n",
      "---質問に対する文書の関連性を確認---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連しています---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連していません---\n",
      "---OKな文書があれば回答を生成---\n",
      "---決定: 生成---\n",
      "---回答生成---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retry_count=0\n",
      "---幻覚を確認---\n",
      "---生成と質問を評価---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---決定: 生成は質問に答えていません---\n",
      "---関連文書の取得---\n",
      "---質問に対する文書の関連性を確認---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連していません---\n",
      "---評価: 文書は関連していません---\n",
      "---OKな文書があれば回答を生成---\n",
      "---決定: すべての文書は質問に関連していません---\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe to store the question, context, and response\n",
    "import time\n",
    "\n",
    "inputs = [\n",
    "{\"question\": \"グローバル・スタートアップ・アクセラレーションプログラムの予算額は？\"},\n",
    "{\"question\": \"ディープテック・スタートアップの起業・経営人材確保等支援事業の予算額は？\"},\n",
    "{\"question\": \"図書館間貸借制度はどのように機能していますか？\"},\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for inp in inputs:\n",
    "    try:\n",
    "        for output in app.stream(inp):\n",
    "            # evaluation_results = {\n",
    "            #     \"document_relevance\": [],\n",
    "            #     \"hallucination_check\": False,\n",
    "            #     \"answers_question\": False\n",
    "            # }\n",
    "            \n",
    "            for key, value in output.items():\n",
    "                if key == \"generate\":\n",
    "                    question = value[\"question\"]\n",
    "                    documents = value[\"documents\"]\n",
    "                    generation = value[\"generation\"]\n",
    "                    \n",
    "                    # コンテキストを文字列のリストとして保持\n",
    "                    # contexts = [doc.page_content for doc in documents]\n",
    "\n",
    "                    # check_relevance = output.get(\"document_relevance\", [])\n",
    "                    # # 評価結果を記録\n",
    "\n",
    "                    # # ドキュメントの関連性（RELEVANT/NOT RELEVANTのリスト）\n",
    "                    # evaluation_results[\"document_relevance\"]\n",
    "                \n",
    "                    \n",
    "                    # # 幻覚チェック結果\n",
    "                    # evaluation_results[\"hallucination_check\"] = \"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\" in str(output)\n",
    "                    \n",
    "                    # # 質問への適合性\n",
    "                    # evaluation_results[\"answers_question\"] = \"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\" not in str(output)\n",
    "                    \n",
    "                    # Append the result with ragas format and evaluation results\n",
    "                    outputs.append({\n",
    "                        \"user_input\": question,\n",
    "                        \"retrieved_contexts\": [doc.page_content for doc in documents],\n",
    "                        \"response\": generation,\n",
    "                        # \"evaluation\": evaluation_results\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        if \"Quota exceeded\" in str(e):\n",
    "            print(\"生成要求のクォータ上限に達しました。後ほど再実行するか、Vertex AIのクォータ増加を申請してください。\")\n",
    "        else:\n",
    "            print(f\"エラーが発生しました: {e}\")\n",
    "        # クォータ超過の場合は、少し待ってから次の質問へ進むか、もしくは処理をスキップ\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate in RAGAS\n",
    "今回は正解データがないのでFaithfulnessのみ\n",
    "詳細 https://docs.ragas.io/en/stable/references/metrics/#ragas.metrics.Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4cf27488b2446dbd8c6facac760828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 1.0000}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "from ragas.metrics import Faithfulness\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(outputs)\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    # metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
    "    metrics=[Faithfulness()],\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
