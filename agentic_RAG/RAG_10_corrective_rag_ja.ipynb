{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Corrective RAG**\n",
    "\n",
    "**修正型検索強化型生成**（ Corrective RAG または CRAG ）: \\\n",
    "検索された文書を使用する前に評価および修正を行うことで、生成された応答の精度を向上させる方法です。その仕組みは以下の通りです。\n",
    "\n",
    "**正しい例：** 関連性のある文書の場合、不必要な部分を削除して精製し、生成に使用します。\n",
    "\n",
    "**誤った例：** 関連性のない文書の場合、破棄し、ウェブ検索を使用するなどして追加情報を取得します。\n",
    "\n",
    "**あいまいな例：** あいまいな場合、システムは取得した情報とウェブ検索した情報を組み合わせて、バランスのとれた応答を作成します。\n",
    "\n",
    "Research Paper: [Corrective RAG](https://arxiv.org/pdf/2401.15884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "# VERTEXAI用の設定\n",
    "import vertexai\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.expanduser(\"~/.config/gcloud/application_default_credentials.json\")\n",
    "vertexai.init(project=os.getenv(\"gcp_project_id\"), location=\"us-central1\")\n",
    "\n",
    "# load llm\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-exp\",\n",
    "    project=os.getenv(\"gcp_project_id\"),\n",
    "    location=\"us-central1\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# # load data\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# loader = PyPDFLoader(\"../data/pdf/57_public_スタートアップ育成に向けた政府の取組_file_name=kaisetsushiryou_2024.pdf\")\n",
    "# documents = loader.load()\n",
    "\n",
    "# # split documents\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# load embedding model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either: - Avoid using tokenizers before the fork if possible - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" # 警告対策　tokenizersライブラリの並列処理を明示的にON \n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "# vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=\"../data/chroma_db_57\")\n",
    "vectorstore = Chroma(persist_directory=\"../data/chroma_db_57\", embedding_function=embeddings) # すでに作ったものを利用\n",
    "\n",
    "# create retirever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ドキュメントグレーダー**\n",
    "ドキュメントグレーダーは、ドキュメントが与えられたクエリに関連しているかどうかを評価します。\n",
    "\n",
    "### Prompt for the grader\n",
    "system = \"\"\"あなたは、検索されたドキュメントがユーザーの質問に関連しているかどうかを評価する採点者です。\n",
    " 厳密なテストである必要はありません。誤った検索結果を除外することが目的です。\n",
    " ドキュメントがユーザーの質問に関連するキーワードまたは意味的な意味を含んでいる場合、関連性があると評価します。\n",
    " ドキュメントが質問に関連しているかどうかを示すために、「はい」または「いいえ」の2値スコアを付けます。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grader for doc retriever \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# langchain_core.pydantic_v1.BaseModel は、LangChain の設定や構成要素を定義・検証するために利用される Pydantic の基盤機能を活かしたモデルクラスです。\n",
    "# Field は、Pydantic モデル内でフィールド（属性）のメタ情報を定義するための関数です。\n",
    "\n",
    "# defining a data class for the grader\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Schema for grading retrieved documents for relevance.\n",
    "    \n",
    "    The field 'binary_score' is expected to be either \"yes\" or \"no\" indicating\n",
    "    whether the document is relevant to the user's question.\n",
    "    \"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments) # LLM （ChatOpenAI）の出力を GradeDocuments の形式に整形するように設定, 構造化出力\n",
    "\n",
    "# Prompt for the grader\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader # 取得された文書とユーザーの質問を用い、LLM が「関連しているか否か」を評価して\"yes\"または\"no\"を返すようになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "# run grader\n",
    "question = \"図書館間貸借制度はどのように機能していますか？\"\n",
    "no_docs = retriever.get_relevant_documents(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": no_docs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# run grader\n",
    "question = \"2024年度における日本のスタートアップによる雇用創出数は？\"\n",
    "yes_docs = retriever.get_relevant_documents(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": yes_docs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document chain\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\"\n",
    "You are a helpful assistant that answers questions based on the following context.'\n",
    "Use the provided context to answer the question.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(yes_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in yes_docs)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024年度における日本のスタートアップによる雇用創出数に関する情報が記載されていません。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response\n",
    "generation = rag_chain.invoke({\"context\": yes_docs, \"question\": question})\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define web search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data class for state\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define graph steps\n",
    "from langchain.schema import Document\n",
    "\n",
    "# node function for retrieval\n",
    "def retrieve(state):\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# node function for generation\n",
    "def generate(state):\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "# node function for check_relevance\n",
    "def grade_documents(state):\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVSNCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "#  node function for web search\n",
    "def web_search(state):\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # web_search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join(d[\"content\"] for d in docs)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# node function for decision\n",
    "def decide_to_generate(state):\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search = state.get(\"web_search\", \"no\").lower()\n",
    "\n",
    "    if web_search == \"yes\":\n",
    "        print(\"---DECISION: WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "\n",
    "# Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade_documents\n",
    "workflow.add_node(\"generate\", generate) # generate\n",
    "workflow.add_node(\"web_search_node\", web_search) # web_search_node\n",
    "\n",
    "# build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "         \"web_search\":  \"web_search_node\",\n",
    "         \"generate\": \"generate\", \n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVSNCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: WEB SEARCH---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search_node':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'図書館利用者の求めに応じて、図書館はその資料を所蔵する他館にその利用を申し込み、所蔵館は無料ないし少ない手数料でそれを貸し出すことで機能しています。\\n'\n"
     ]
    }
   ],
   "source": [
    "# example 1 where documents are relevant\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"図書館間貸借制度はどのように機能していますか？\"}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "pprint(value['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVSNCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'スタートアップは、雇用創出にも大きな役割を果たすと述べられています。\\n'\n"
     ]
    }
   ],
   "source": [
    "# example 2 where documents are not relevant\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"日本のスタートアップによる雇用創出作用は？\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVSNCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---GENERATE---\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVSNCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---GENERATE---\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVSNCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n"
     ]
    }
   ],
   "source": [
    "# 複数の質問を準備\n",
    "input_questions = [\n",
    "    {\"question\": \"令和五年ウクライナ復興支援事業予算は\"},\n",
    "    {\"question\": \"日本のスタートアップによる雇用創出作用は？一行で答えて\"},\n",
    "    {\"question\": \"図書館間貸借制度はどのように機能していますか？一行で答えて\"},\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "expected_responses = {\n",
    "    \"令和五年ウクライナ復興支援事業予算は\": \"2024年9月時点の資料では260億円となっています。\",\n",
    "    \"日本のスタートアップによる雇用創出作用は？一行で答えて\": \"日本のスタートアップは、新たな産業と雇用を生み出し、経済の活性化に貢献しています。\",\n",
    "    \"図書館間貸借制度はどのように機能していますか？一行で答えて\": \"図書館間貸借制度とは、図書館同士が協力し、利用者の求めに応じて互いの所蔵資料を貸し借りする仕組みです。\",\n",
    "} # あらかじめ https://gemini.google.com/app で gemini-2.0-flash にきいておいた答え\n",
    "\n",
    "# 各質問に対して処理を実行\n",
    "for inputs in input_questions:\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            if key == \"generate\":\n",
    "                question = value[\"question\"]\n",
    "                documents = value[\"documents\"]\n",
    "                generation = value[\"generation\"]\n",
    "\n",
    "                context = \"\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "                # Append the result\n",
    "                outputs.append({\n",
    "                    \"query\": question,\n",
    "                    \"context\": context,\n",
    "                    \"response\": generation,\n",
    "                    \"expected_response\": expected_responses.get(question, \"\")  # 期待される回答がない場合は空文字を返す\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>expected_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>令和五年ウクライナ復興支援事業予算は</td>\n",
       "      <td>価制度の導入予定【R６年度より導入予定】、デジタルマーケットプレイ\\nスの本格稼働【R６年度...</td>\n",
       "      <td>令和5年補正予算におけるウクライナ復興支援事業は260億円(経産)です。\\n</td>\n",
       "      <td>2024年9月時点の資料では260億円となっています。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本のスタートアップによる雇用創出作用は？一行で答えて</td>\n",
       "      <td>スタートアップとは\\n① スタートアップとは、一般に、以下のような企業をいう。\\n1. 新し...</td>\n",
       "      <td>スタートアップは、雇用創出にも大きな役割を果たすとされています。\\n</td>\n",
       "      <td>日本のスタートアップは、新たな産業と雇用を生み出し、経済の活性化に貢献しています。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>図書館間貸借制度はどのように機能していますか？一行で答えて</td>\n",
       "      <td>... かの図書館同士の相互貸借制度が整っています。 利用者の借りたい本を、他の図書館から最...</td>\n",
       "      <td>図書館間貸借制度は、利用者の求めに応じて、図書館が他の図書館に資料の利用を申し込み、所蔵館が...</td>\n",
       "      <td>図書館間貸借制度とは、図書館同士が協力し、利用者の求めに応じて互いの所蔵資料を貸し借りする仕...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           query  \\\n",
       "0             令和五年ウクライナ復興支援事業予算は   \n",
       "1    日本のスタートアップによる雇用創出作用は？一行で答えて   \n",
       "2  図書館間貸借制度はどのように機能していますか？一行で答えて   \n",
       "\n",
       "                                             context  \\\n",
       "0  価制度の導入予定【R６年度より導入予定】、デジタルマーケットプレイ\\nスの本格稼働【R６年度...   \n",
       "1  スタートアップとは\\n① スタートアップとは、一般に、以下のような企業をいう。\\n1. 新し...   \n",
       "2  ... かの図書館同士の相互貸借制度が整っています。 利用者の借りたい本を、他の図書館から最...   \n",
       "\n",
       "                                            response  \\\n",
       "0             令和5年補正予算におけるウクライナ復興支援事業は260億円(経産)です。\\n   \n",
       "1                 スタートアップは、雇用創出にも大きな役割を果たすとされています。\\n   \n",
       "2  図書館間貸借制度は、利用者の求めに応じて、図書館が他の図書館に資料の利用を申し込み、所蔵館が...   \n",
       "\n",
       "                                   expected_response  \n",
       "0                        2024年9月時点の資料では260億円となっています。  \n",
       "1          日本のスタートアップは、新たな産業と雇用を生み出し、経済の活性化に貢献しています。  \n",
       "2  図書館間貸借制度とは、図書館同士が協力し、利用者の求めに応じて互いの所蔵資料を貸し借りする仕...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(outputs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
