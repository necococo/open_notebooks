{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vozCqGmy6GNs",
        "outputId": "fdeb9ca2-2dd3-4eeb-9395-e5e998b00841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このノートブックは、Unsloth というライブラリの使い方を説明するチュートリアルです。Unsloth は、Google Colab 上で大きな言語モデル (LLM) を効率的に fine-tuning するためのツールです。オリジナルは[こちら](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)\n",
        "\n",
        "特に4bit QLoRAは、4ビット量子化によりモデルを軽量化し、その上でLoRAを用いて低ランクのアダプタのみを学習することで、通常の高精度ファインチューニングと同等の性能を、はるかに少ないリソースで実現する技術です。これにより、個人ユーザーでも限られたGPU環境で大規模言語モデルの微調整が可能となります。\n",
        "\n",
        "# 学習内容\n",
        "- このノートブックでは、以下の内容を学ぶことができます。\n",
        "\n",
        "## データの準備:\n",
        "- Fine-tuning 用のデータセットを準備する方法。ここでは、Phi-4 というフォーマットで会話を表現し、[FineTome-100k データセット](https://huggingface.co/datasets/mlabonne/FineTome-100k)を使用しています。対話形式のインストラクションファインチューニング向けの大規模なデータセットです。\n",
        "\n",
        "## モデルのトレーニング:\n",
        "- LoRA (Low-Rank Adaptation) を使用して、モデルを効率的に fine-tuning する方法。Huggingface TRL の SFTTrainer を使用し、train_on_completions メソッドでアシスタントの出力のみをトレーニングします。\n",
        "    - TRL（Transformers Reinforcement Learning）は大規模言語モデルをよりユーザに沿った形に調整するための「強化学習」側面にフォーカスしたツールキットと言えます。\n",
        "    - SFTTrainerはTRLライブラリ内で提供される教師あり微調整（Supervised Fine-Tuning, SFT）のためのトレーナークラスです。このクラスは、事前学習済みの言語モデルをユーザーが用意した指示応答データ（例：プロンプトと正解のペア）で微調整する際に利用されます。\n",
        "    - 通常のSFTでは、プロンプトと正解応答のペアを用いて損失（例：クロスエントロピー）を計算します。train_on_completionsは、既に生成されたcompletionデータを入力として受け取り、これらのcompletionとターゲット（正解）との間で損失を計算し、モデルパラメータの更新を行う処理を実装しています。\n",
        "    - 一般的に「completion」とは、モデルがプロンプトに対して生成したテキストのことを指します。chatの回答もその一つです。\n",
        "\n",
        "## 推論:\n",
        "- Fine-tuning されたモデルを使って推論を実行する方法。min_p と temperature を設定してテキストを生成したり、TextStreamer を使用して連続的な推論を行うことができます。\n",
        "\n",
        "## モデルの保存:\n",
        "- Fine-tuning されたモデルを LoRA アダプターとして保存する方法。Huggingface の push_to_hub (公開) または save_pretrained (ローカル保存) を使用します。また、float16 や GGUF 形式で保存する方法も説明されています。\n",
        "\n",
        "## 要約\n",
        "- このノートブックは、Unsloth を使用して Google Colab 上で LLM を fine-tuning し、推論を実行し、モデルを保存する方法を学ぶためのチュートリアルです。\n",
        "\n",
        "### その他の情報\n",
        "- さらに、ノートブックでは以下のような情報も提供されています。\n",
        "\n",
        "    - Unsloth のインストール方法\n",
        "    - 使用できる事前量子化モデルのリスト\n",
        "    - LoRA アダプターの設定\n",
        "    - ShareGPT スタイルのデータセットを HuggingFace の標準フォーマットに変換する方法\n",
        "    - メモリ使用量とトレーニング時間の統計\n",
        "    - VLLM や llama.cpp で使用するためのモデルの保存方法\n",
        "    - Unsloth の Discord チャンネルへのリンク\n",
        "\n",
        "### 以下は各用語の概要です。\n",
        "- GGUF\n",
        "  -  LLMの量子化モデルを保存するための新しいファイルフォーマットです。\n",
        "  -  従来のGGMLフォーマットを進化させ、より柔軟で拡張性の高いメタデータや効率的な読み込みを実現することを目的としています。\n",
        "- vLLM\n",
        "  -  大規模言語モデル（LLM）の推論を高速化し、モデルのサービングを簡単かつ効率的に行えるようにするためのオープンソースライブラリです。PagedAttentionを活用しており、HuggingFace Transformersと比較して、モデルアーキテクチャの変更を一切必要とせずに、最大24倍の処理能力を実現します。\n",
        "- bfloat16\n",
        "    - 米Google（グーグル）が独自に定義したディープラーニング向けの浮動小数点フォーマット。 同社のディープラーニングアクセラレーター「TPU」で採用されている形式。"
      ],
      "metadata": {
        "id": "O3YXIaIl0DG_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHs9XOmjycyE"
      },
      "source": [
        "これらを実行するには、「Runtime」を押して、空いているTesla T4 Google Colabインスタンスで「Run all」を押してください！\n",
        "\n",
        "Unslothを自分のコンピューターにインストールするには、[こちら](https://docs.unsloth.ai/get-started/installing-+-updating)のGithubページのインストール手順に従ってください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ht-qzQycyF"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51rPbed4ycyF"
      },
      "source": [
        "**推論モデルのトレーニング方法については、[当社のブログ記事](https://unsloth.ai/blog/r1-reasoning)をご覧ください。**\\\n",
        "すべての[モデルアップロード](https://docs.unsloth.ai/get-started/all-our-models)および[ノートブック](https://docs.unsloth.ai/get-started/unsloth-notebooks)については、当社のドキュメントをご覧ください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFXQLEnPycyG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syr3X4mSycyG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFI27qHZycyG"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel  # FastVisionModel for LLMs\n",
        "import torch\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 2x faster\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b 2x faster!\n",
        "    \"unsloth/Phi-4\",  # Phi-4 2x faster!\n",
        "    \"unsloth/Phi-4-unsloth-bnb-4bit\",  # Phi-4 Unsloth Dynamic 4-bit Quant\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",  # Gemma 2x faster!\n",
        "    \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Qwen 2.5 2x faster!\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "]  # More models at https://docs.unsloth.ai/get-started/all-our-models\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-4\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "メモ：\n",
        "\n",
        "| モデル名                                        | 日本語理解・生成         | 訓練データ          | 使用例      | 対応度       |\n",
        "|-------------------------------------------------|--------------------------|---------------------|-------------|--------------|\n",
        "| unsloth/Meta-Llama-3.1-8B-bnb-4bit              | 限定的                   | 可能性あり          | 評価未報告  | 限定的       |\n",
        "| unsloth/Mistral-Small-Instruct-2409             | 良好                     | 多言語対応          | 良好        | 良好         |\n",
        "| unsloth/Phi-4                                   | ほぼ不可                 | 英語中心            | 評価なし    | 非対応       |\n",
        "| unsloth/Phi-4-unsloth-bnb-4bit                  | ほぼ不可                 | 英語中心            | 評価なし    | 非対応       |\n",
        "| unsloth/gemma-2-9b-bnb-4bit                     | ほぼ不可                 | 英語中心            | 評価なし    | 非対応       |\n",
        "| unsloth/Qwen2.5-7B-Instruct-bnb-4bit            | 良好                     | 多言語対応          | 良好        | 良好         |\n",
        "| unsloth/Llama-3.2-1B-bnb-4bit                   | 限定的                   | 少量含む可能性      | 評価未報告  | 限定的       |\n",
        "| unsloth/Llama-3.2-1B-Instruct-bnb-4bit          | 限定的                   | 公式外              | 評価なし    | 限定的       |\n",
        "| unsloth/Llama-3.2-3B-bnb-4bit                   | 限定的                   | 少量含む可能性      | 評価なし    | 限定的       |\n",
        "| unsloth/Llama-3.2-3B-Instruct-bnb-4bit          | 限定的                   | 公式外              | 評価なし    | 限定的       |"
      ],
      "metadata": {
        "id": "tmIr4x1x25wD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "パラメータ効率の微調整を行うために、LoRAアダプターを追加しました。これにより、すべてのパラメータの1%のみを効率的にトレーニングできるようになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "ac0093c3-162b-4b59-c342-32d6f67bfaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.2.15 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "### データ準備\n",
        "現在、会話形式のファインチューニングには Phi-4 フォーマットを使用しています。[Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)  データセットを ShareGPT スタイルで利用していますが、(\"from\", \"value\") ではなく、HuggingFace の通常のマルチターン形式である (\"role\", \"content\") に変換しています。Phi-4 は以下のようにマルチターンの会話をレンダリングします:\n",
        "\n",
        "```\n",
        "<|im_start|>user<|im_sep|>Hello!<|im_end|>\n",
        "<|im_start|>assistant<|im_sep|>Hi! How can I help?<|im_end|>\n",
        "<|im_start|>user<|im_sep|>What is 2+2?<|im_end|>\n",
        "```\n",
        "\n",
        "正しいチャットテンプレートを取得するために、`get_chat_template` 関数を使用しています。 `zephyr、chatml、mistral、llama、alpaca、vicuna、vicuna_old、phi3、phi4、llama3` などに対応しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-4\",\n",
        ") # \"phi-4\" というチャットテンプレートに基づいて tokenizer を設定します。これにより、tokenizer は Phi-4 フォーマットの会話を処理できるようになります。\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"この関数は、データセットの各サンプル（examples）に対して、\"conversations\" カラムから会話を抽出し、tokenizer.apply_chat_template を使用して Phi-4 フォーマットのテキストに変換します。そして、変換されたテキストを \"text\" というキーを持つ辞書として返します。\n",
        "    \"\"\"\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "\n",
        "    return { \"text\" : texts, }\n",
        "# pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "次に、`standardize_sharegpt`を使用して、ShareGPTスタイルのデータセットをHuggingFaceの汎用フォーマットに変換します。これにより、データセットは次のようになります。\n",
        "\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "その後、先程のformatting_prompts_funcを適用してtextキーにtext valueを入れます。"
      ],
      "metadata": {
        "id": "YwKHU4rjIH1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPXzJZzHEgXe"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")\n",
        "\"\"\"formatting_prompts_func をデータセットの各サンプルに適用します。 batched=True を指定することで、処理がバッチ化され、効率的に実行されます。この処理によって、各サンプルに \"text\" という新しいカラムが追加され、Phi-4 フォーマットのテキストが格納されます。\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "項目5の会話がどのように構成されているかを見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[{'content': '天文学者は、ドップラー効果を利用して天体の速度を測定するために必要な、静止している天体が放つ光の元の波長をどのようにして決定するのですか？',\n",
        "  'role': 'user'},\\\n",
        " {'content': '天文学者は、星に含まれる元素の独特なスペクトル指紋を利用します。これらの元素は、特定の既知の波長で光を放出および吸収し、吸収スペクトルを形成します。遠方の恒星から届く光を分析し、実験室で測定したこれらの元素のスペクトルと比較することで、天文学者はドップラー効果によるこれらの波長の変化を特定することができます。観測された変化から、光がどの程度赤方偏移または青方偏移しているかがわかり、それによって地球に対する視線方向の恒星の速度を計算することができます。\n",
        "  'role': 'assistant'}]"
      ],
      "metadata": {
        "id": "HG_am69ipuvQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfzTdMtvGE6w"
      },
      "source": [
        "そして、チャットテンプレートがこれらの会話をどのように変えたかを見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhXv0xFMGNKE"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<|im_start|>user<|im_sep|>天文学者は、ドップラー効果を利用して天体の速度を測定するために必要な、静止している天体が放つ光の元の波長をどのようにして決定するのですか？<|im_end|>\\\n",
        "<|im_start|>assistant<|im_sep|>天文学者は、恒星に含まれる元素の固有のスペクトルフィンガープリントを利用します。これらの元素は、特定の既知の波長で光を放出および吸収し、吸収スペクトルを形成します。遠方の星から届く光を分析し、実験室で測定したこれらの元素のスペクトルと比較することで、天文学者はドップラー効果によるこれらの波長の変化を特定することができます。観測された変化から、光がどの程度赤方偏移または青方偏移したかがわかり、それによって地球に対する視線に沿った星の速度を計算することができます。<|im_end|>"
      ],
      "metadata": {
        "id": "5rCQkSWIp7Hm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "### モデルをトレーニングする\n",
        "では、Huggingface TRLの `SFTTrainer` を使ってみましょう！ 詳しい説明はこちら： [TRL SFT ドキュメント](https://huggingface.co/docs/trl/sft_trainer)。 \\\n",
        "処理を高速化するために 60 ステップを実行しますが、フル実行の場合は `num_train_epochs=1` を設定し、`max_steps=None` をオフにします。TRLの `DPOTrainer` もサポートしています！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "また、Unslothの `train_on_completions` メソッドを使用して、アシスタントの出力のみを学習し、ユーザーの入力の損失を無視します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<|im_start|>user<|im_sep|>\",\n",
        "    response_part=\"<|im_start|>assistant<|im_sep|>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "マスキングが実際に実行されていることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtsMVtlkUhja"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]) # input_ids は、テキストがトークン化され、モデルが理解できる数値IDに変換されたものでそれを元のテキストにデコードしています。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<|im_start|>user<|im_sep|>天文学者は、ドップラー効果を利用して天体の速度を測定するために必要な、静止している天体が放つ光の元の波長をどのようにして決定するのですか？<|im_end|>\\\n",
        "<|im_start|>assistant<|im_sep|>天文学者は、恒星に含まれる元素の固有のスペクトルフィンガープリントを利用します。これらの元素は、特定の既知の波長で光を放出および吸収し、吸収スペクトルを形成します。遠方の星から届く光を分析し、実験室で測定したこれらの元素のスペクトルと比較することで、天文学者はドップラー効果によるこれらの波長の変化を特定することができます。観測された変化から、光がどの程度赤方偏移または青方偏移したかがわかり、それによって地球に対する視線に沿った星の速度を計算することができます。<|im_end|>"
      ],
      "metadata": {
        "id": "ULvRnd13rMNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rD6fl8EUxnG"
      },
      "outputs": [],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0] # 空白 (\" \") をトークン化し、その input_idsの0番目を space 変数に格納します\n",
        "print(space)\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]) # 5番目のサンプルのラベルデータを取得します。ラベルデータは、モデルが予測すべき出力（ここではアシスタントの発言）に対応します。 train_on_responses_only関数ではこのようにユーザーの発言をマスク（-100) で置き換えています。（spaceに置き換えて出力してみることで確認できました）"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "天文学者は、恒星に含まれる元素の独特なスペクトル指紋を利用しています。これらの元素は、特定の既知の波長で光を放出および吸収し、吸収スペクトルを形成します。遠方の星から届く光を分析し、実験室で測定したこれらの元素のスペクトルと比較することで、天文学者はドップラー効果によるこれらの波長の変化を特定することができます。観測された変化から、光がどの程度赤方偏移または青方偏移したかがわかり、それによって地球に対する視線に沿った星の速度を計算することができます。<|im_end|>"
      ],
      "metadata": {
        "id": "_Ieoh993rYTl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enWUM0jV-jV"
      },
      "source": [
        "システムとインストラクションのプロンプトが正しくマスクされていることがわかります！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "468b4d16-9636-4d1d-d5bd-e53ec75067e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "9.967 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) # model, tokenizer, dataset, ライブラリが内部的に使用するメモリなど\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "1af97423-1dd9-4368-a33a-3355d5c94950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "991.7127 seconds used for training.\n",
            "16.53 minutes used for training.\n",
            "Peak reserved memory = 12.42 GB.\n",
            "Peak reserved memory for training = 2.453 GB.\n",
            "Peak reserved memory % of max memory = 84.255 %.\n",
            "Peak reserved memory for training % of max memory = 16.641 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "### 推論\n",
        "モデルを実行してみましょう！ 命令と入力を変更できます。出力は空白のままにしてください！\n",
        "\n",
        "**[NEW] Llama-3.1 8b Instructの無料Colabで2倍高速な推論を試してみましょう。[こちら](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-4\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# アテンションマスクを作成\n",
        "attention_mask = torch.ones_like(inputs)\n",
        "attention_mask[inputs == tokenizer.pad_token_id] = 0\n",
        "\n",
        "# モデルにアテンションマスクを渡す\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    attention_mask=attention_mask.to(\"cuda\"),\n",
        "    max_new_tokens=64,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1,\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jjthdlRQ2OS",
        "outputId": "eb9f1258-799b-486d-a5a5-4ecf1e8db5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|im_start|>user<|im_sep|>Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|><|im_start|>assistant<|im_sep|>The next number in the Fibonacci sequence is 13. The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones, usually starting with 0 and 1. In this case, the sequence starts with 1 and 1, and each subsequent number is the sum of the']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "['<|im_start|>user<|im_sep|>フィボナッチ数列を続ける：1、1、2、3、5、8、<|im_end|>\\\n",
        "<|im_start|>assistant<|im_sep|>フィボナッチ数列の次の数は13です。\\\n",
        "フィボナッチ数列とは、各数が直前の2つの数の和となる一連の数字であり、通常は0と1から始まります。この場合、1と1から始まり、その後の各数は直前の2つの数の和となります。]"
      ],
      "metadata": {
        "id": "2H-snm0oswng"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " また、`TextStreamer` を使用して連続して推論を行うこともできます。これにより、生成をトークンごとに確認できるため、全体を待つ必要がなくなります！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "ecbc1635-fcbf-4d93-dd89-5d5aebf550c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The next number in the Fibonacci sequence is 13. The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones, usually starting with 0 and 1. In this case, the sequence starts with 1 and 1, and each subsequent number is the sum of the two preceding numbers. So, the next number after 8 is 13, which is the sum of 5 and 8.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(\n",
        "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "フィボナッチ数列の次の数は13です。フィボナッチ数列とは、通常0と1から始まり、各数が直前の2つの数の合計となる一連の数です。この場合、数列は1と1から始まり、その後の各数は直前の2つの数の合計です。したがって、8の次の数は13であり、これは5と8の合計です。<|im_end|>"
      ],
      "metadata": {
        "id": "7XYgrC2ptaAv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "### 調整したモデルの保存と読み込み\n",
        "最終モデルをLoRAアダプターとして保存するには、Huggingfaceの `push_to_hub` を使用してオンラインで保存するか、`save_pretrained` を使用してローカルで保存します。\n",
        "\n",
        "**[注意]** これはLoRAアダプターのみを保存し、完全なモデルは保存しません。16ビットまたはGGUFに保存するには、下にスクロールしてください！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "2fb22f9f-a610-40df-90e8-1f6faca32361"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model/vocab.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model/merges.txt',\n",
              " '/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "推論用に保存したばかりのLoRAアダプターをロードしたい場合は、`False`を`True`に設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False: # コードブロック全体の実行を無効化しています。\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(\n",
        "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "エッフェル塔は、フランスの首都パリにある高い塔です。高さは324メートル（1,063フィート）で、1889年に完成しました。この塔は、フランス革命100周年を祝う1889年の万国博覧会のために、ギュスターヴ・エッフェルとそのチームによって設計されました。錬鉄でできており、3つのレベルから構成され、観光客が訪れることができます。エッフェル塔は世界で最も有名なランドマークのひとつであり、毎年何百万人もの観光客が訪れます。<|im_end|>"
      ],
      "metadata": {
        "id": "Yf8jpAdztvXF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "Hugging FaceのAutoModelForPeftCausalLMも使用できます。unslothがインストールされていない場合のみ、これを使用してください。4bitモデルのダウンロードがサポートされていないため、処理が非常に遅くなる可能性があります。また、Unslothの推論は2倍高速です。(Peft: Parameter efficient fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model/lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model/lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### VLLM用のfloat16への保存\n",
        "\n",
        "また、`float16` への直接保存もサポートしています。float16 には `merged_16bit` を、int4 には `merged_4bit` を選択してください。また、`lora` アダプターをフォールバックとして使用することもできます。Hugging Face アカウントにアップロードするには、`push_to_hub_merged` を使用してください！ 個人用トークンは https://huggingface.co/settings/tokens から入手できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp 変換\n",
        "`GGUF` / `llama.cpp` に保存するには、現在ネイティブでサポートしています。`llama.cpp` をクローンし、デフォルトで `q8_0` に保存します。`q4_k_m` のようなすべてのメソッドを許可します。ローカル保存には `save_pretrained_gguf` を、HF へのアップロードには `push_to_hub_gguf` を使用します。\n",
        "\n",
        "サポートされている量子化手法（全リストは [Docs](https://docs.unsloth.ai/basics/saving-and-using-models/saving-to-gguf) を参照）：\n",
        "* `q8_0` - 高速変換。リソースの使用量は多いが、概ね許容できる。\n",
        "* `q4_k_m` - 推奨。attention.wvとfeed_forward.w2のテンソルの半分にQ6_Kを使用し、それ以外にはQ4_Kを使用します。\n",
        "* `q5_k_m` - 推奨。attention.wvとfeed_forward.w2のテンソルの半分にQ6_Kを使用し、それ以外にはQ5_Kを使用します。\n",
        "\n",
        "[**NEW**] Ollamaへの微調整と自動エクスポートを行うには、[Ollamaノートブック](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)をお試しください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"/content/drive/MyDrive/Colab Notebooks/weights/Phi_4/model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BoSslJIycyL"
      },
      "source": [
        "次に、`model-unsloth.gguf`ファイルまたは`model-unsloth-Q4_K_M.gguf`ファイルをllama.cppまたはJanやOpen WebUIのようなUIベースのシステムで使用します。Janは[こちら](https://github.com/janhq/jan)から、Open WebUIは[こちら](https://github.com/open-webui/open-webui)からインストールできます\n",
        "\n",
        "Unslothについて何か質問があれば、[Discord](https://discord.gg/unsloth)チャンネルをご利用ください！バグを見つけた場合や、LLMの最新情報を入手したい場合、またはヘルプが必要な場合、プロジェクトに参加したい場合など、Discordにぜひご参加ください！\n",
        "\n",
        "その他のリンク：\n",
        "1. Llama 3.2 会話型ノートブック。 [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Ollamaへの微調整の保存。[フリーノートブック](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 ビジョン微調整 - 放射線画像のユースケース。 [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. DPO、ORPO、継続的な事前トレーニング、会話の微調整などについては、[ドキュメント](https://docs.unsloth.ai/get-started/unsloth-notebooks)のノートブックをご覧ください！\n",
        "\n",
        "<div class=「align-center」>\n",
        "  <a href=「https://unsloth.ai」><img src=「https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png」 width=「115」></a>\n",
        "  <a href=「https://discord.gg/unsloth」><img src=「https://github.com/unslothai/unsloth/raw/main/images/Discord.png」 width=「145」></a>\n",
        "  <a href=「https://docs.unsloth.ai/」><img src=「https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true」 width=「125」>\n",
        "</div>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}